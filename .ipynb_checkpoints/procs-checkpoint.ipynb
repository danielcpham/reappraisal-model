{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- When running on Google Colab, mount Google Drive to access scripts.\n",
    "- `cd` into the project root and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# ROOT_DIR = \"/content/drive/MyDrive/ldh\"\n",
    "\n",
    "# %pip install transformers datasets pytorch-lightning \"ray[tune]\" wandb\n",
    "# %pip uninstall dataclasses # Apparently we need to remove dataclasses or it messes with the tuner\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "ROOT_DIR = \"/Users/danielpham/Google Drive/ldh\"\n",
    "%cd {ROOT_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define constants\n",
    "STRAT = 'obj'\n",
    "BATCH_SIZE = 16 if not torch.cuda.is_available() else 16\n",
    "TEST_BATCH_SIZE = 64\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDH Data\n",
    "\n",
    "Contains the following:\n",
    "\n",
    "- LDHI\n",
    "- LDHII\n",
    "- LDHIII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "ldhdata = LDHDataModule(data_dir=ROOT_DIR, batch_size=BATCH_SIZE, strat=STRAT, kfolds=NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run K-Fold Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from reappraisalmodel.trainers import kfold_train\n",
    "\n",
    "results = kfold_train(NUM_FOLDS, ldhdata, strat=STRAT, max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from reappraisalmodel.trainers import run_tune\n",
    "run_tune(ldhdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(pretrained_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Returns a BatchEncoding of the text.\n",
    "tokenized = tokenizer(text = [\"This is the first test sentence!\", \"This is the second, better test sentence.\"], \n",
    "    padding='max_length', max_length=150)\n",
    "\n",
    "for idx, sent in enumerate(tokenized.input_ids):\n",
    "    print(f\"Sentence            {idx}: {tokenizer.convert_ids_to_tokens(sent)}\")\n",
    "    print(f\"Tokenized Attention {idx}: {tokenized[idx].attention_mask}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_config = default_config = {\n",
    "    'lr': 1e-3,\n",
    "    'hidden_layer_size': 50\n",
    "}\n",
    "\n",
    "# Convert attention mask to a true/false tensor\n",
    "# use masked_select to select only the non-zero parts of the encoding\n",
    "# Average over feature dimension using AvgPool1d, where stride_size = kernel_size = encode_length_no_zeros\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningReapp.load_from_checkpoint(\n",
    "    '/Users/danielpham/Google Drive/ldh/lightning_logs_obj_0223/version_2/checkpoints/epoch=1-step=337.ckpt', map_location='cpu')\n",
    "\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer \n",
    "\n",
    "model.eval()\n",
    "trainer = Trainer(\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "test_dataloader = ldhdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for result in results:\n",
    "    print(len(result['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./output_reapp.pkl', 'rb+') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'ldhdata'\n",
    "file = 'Master_Final_TrainingData.csv'\n",
    "\n",
    "s3client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3client.get_object(Bucket=bucket, Key=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encodings.utf_8.StreamReader"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs \n",
    "import csv\n",
    "\n",
    "train = csv.DictReader(codecs.getreader(\"utf-8\")(response[\"Body\"])) # returns an ordered dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
