{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- When running on Google Colab, mount Google Drive to access scripts.\n",
    "- `cd` into the project root and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "\n",
    "# Define constants\n",
    "STRAT = 'obj'\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDH Data\n",
    "\n",
    "Contains the following:\n",
    "\n",
    "- LDHI\n",
    "- LDHII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at output/training/obj/cache-50ed3c54936a704e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded from disk.\n",
      "Encoding Training Data:\n",
      "Evaluation data loaded from disk.\n",
      "Encoding Test Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b47830fd6c4f7d8ea8a5b32a7d5e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32109.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "\n",
    "ROOT_DIR = pathlib.Path().parent\n",
    "ldhdata = LDHDataModule(data_dir=ROOT_DIR, strat=STRAT)\n",
    "ldhdata.load_train_data()\n",
    "ldhdata.load_eval_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run K-Fold Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory reapp_logs exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158513cadbcc479280a16eff2b9e197d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373244a7719e49d6b595c8892f09e6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger, WandbLogger\n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "from reappraisalmodel.utils import upload_file\n",
    "\n",
    "import datetime\n",
    "\n",
    "strat = 'far'\n",
    "config = {\n",
    "    'lr': 1e-3,\n",
    "    'num_embedding_layers': 2,\n",
    "    'batch_size': 128\n",
    "}\n",
    "save_dir=ROOT_DIR / 'reapp_logs'\n",
    "today = datetime.datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "name=f\"_{strat}_{today}\"\n",
    "\n",
    "\n",
    " # Loggers\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=save_dir,\n",
    "    name=name,\n",
    "\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir=save_dir,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "    #Checkpoints\n",
    "early_stop_checkpoint = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    min_delta=0.01,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    dirpath=save_dir,\n",
    "    filename= '{epoch:02d}-{val_loss:.02f}',\n",
    "    verbose=True,\n",
    "    save_last=False,\n",
    "    save_top_k=1,\n",
    "    save_weights_only=False,\n",
    ")\n",
    "\n",
    "model = LightningReapp(config)\n",
    "trainer = lit.Trainer(\n",
    "    benchmark=True,\n",
    "    logger = [logger, csv_logger],\n",
    "    gpus = 1,\n",
    "    val_check_interval=0.25,\n",
    "    gradient_clip_val=0.5,\n",
    "    max_epochs=10,\n",
    "    terminate_on_nan=True,\n",
    "    weights_summary=None,\n",
    "    callbacks=[callback_checkpoint, early_stop_checkpoint])\n",
    "trainer.fit(model, ldhdata.get_train_dataloader(batch_size=model.batch_size), \n",
    "ldhdata.get_val_dataloader(batch_size=model.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['r2score'] = df['r2score'].apply(lambda x: x.item())\n",
    "df['explained_var'] = df['explained_var'].apply(lambda x: x.item())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import pytorch_lightning as lit\n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "model = LightningReapp({\n",
    "    'lr': 1e-3,\n",
    "    'hidden_layer_size': 50\n",
    "})\n",
    "\n",
    "trainer = lit.Trainer(fast_dev_run=1)\n",
    "trainer.fit(model, ldhdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.list_objects(Bucket='ldhdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"obj/{'metrics': {'epoch': tensor(10.), 'val_loss': tensor(1.2882), 'r2score': tensor(0.5832), 'explained_var': tensor(0.6109), 'train_loss': tensor(1.1194, device='cuda:0')}, 'checkpoint': '/tmp/tmpvsrtlvfl/reappmodel_obj_20210227_203013/2_epoch=07-val_loss=1.12.ckpt', 'num_epochs': 10}-20210227_203013-2_epoch=07-val_loss=1.12.ckpt\"\n",
    "s3.copy({\n",
    "    'Bucket': 'ldhdata',\n",
    "    'Key': key\n",
    "},\n",
    "Bucket='ldhdata',\n",
    "Key='obj/20210227_203013-2_epoch=07-val_loss=1.12.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "metrics = [\n",
    "    {'epoch': torch.tensor(7.), 'val_loss': 1.1842, 'r2score': torch.tensor(0.6130), 'explained_var': torch.tensor(0.6388), 'train_loss': torch.tensor(1.1642, device='cuda:0')},\n",
    "    {'epoch': torch.tensor(8.), 'val_loss': 1.1819, 'r2score': torch.tensor(0.6087), 'explained_var': torch.tensor(0.6377), 'train_loss': torch.tensor(1.0931, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(9.), 'val_loss': 1.2094, 'r2score': torch.tensor(0.5926), 'explained_var': torch.tensor(0.6366), 'train_loss': torch.tensor(1.1363, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2712, 'r2score': torch.tensor(0.5906), 'explained_var': torch.tensor(0.6339), 'train_loss': torch.tensor(1.0842, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2882, 'r2score': torch.tensor(0.5832), 'explained_var': torch.tensor(0.6109), 'train_loss': torch.tensor(1.1194, device='cuda:0')}, \n",
    "]\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "for key in ['r2score', 'epoch', 'explained_var', 'train_loss']:\n",
    "    df[key] = df[key].apply(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_report = upload_file('this.csv', 'ldhdata', f'obj/20210227_203013-report.csv')\n",
    "print(f\"Successful Uploading Report to s3: {upload_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in resp['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Returns a BatchEncoding of the text.\n",
    "tokenized = tokenizer(text = [\"This is the first test sentence!\", \"This is the second, better test sentence.\"], \n",
    "    padding='max_length', max_length=150)\n",
    "\n",
    "for idx, sent in enumerate(tokenized.input_ids):\n",
    "    print(f\"Sentence            {idx}: {tokenizer.convert_ids_to_tokens(sent)}\")\n",
    "    print(f\"Tokenized Attention {idx}: {tokenized[idx].attention_mask}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_config = default_config = {\n",
    "    'lr': 1e-3,\n",
    "    'hidden_lay': 50\n",
    "}\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer = lit.Trainer(\n",
    "    gpus = 1 if torch.cuda.is_available() else None,\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    max_epochs=10,\n",
    "    fast_dev_run=2,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer.fit(model, ldhdata.train_dataloader(), ldhdata.val_dataloader())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningReapp.load_from_checkpoint(\n",
    "    '/Users/danielpham/Google Drive/ldh/lightning_logs_obj_0223/version_2/checkpoints/epoch=1-step=337.ckpt', map_location='cpu')\n",
    "\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload \n",
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer \n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "config = {\n",
    "    'lr': 1e-3,\n",
    "    'num_embedding_layers': 2\n",
    "}\n",
    "\n",
    "model = LightningReapp(config)\n",
    "trainer = Trainer(\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "test_dataloader = ldhdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for result in results:\n",
    "    print(len(result['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'ldhdata'\n",
    "file = 'Master_Final_TrainingData.csv'\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "response = s3client.get_object(Bucket=bucket, Key=file)\n",
    "\n",
    "import codecs \n",
    "import csv\n",
    "\n",
    "train = csv.DictReader(codecs.getreader(\"utf-8\")(response[\"Body\"])) # returns an ordered dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
