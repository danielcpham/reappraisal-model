{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training For Linguistic Distancing and Emotion Regulation\n",
    "\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "> pipenv shell  #Generates a new virtual environment based on Pipfile\n",
    "> pipenv install # Installs the packages in Pipfile.lock (Use --dev) to also install dev packages\n",
    "```\n",
    "## Included Datasets\n",
    "- LDHII \n",
    "- \n",
    "- Emobank\n",
    "**Sources**\n",
    "-  [Sentiment Analysis Text Classification Tutorial](https://www.youtube.com/watch?v=8N-nM3QW7O0)\n",
    "- [Using Catalyst for Training Organization](https://github.com/catalyst-team/catalyst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pip install transformers datasets nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No GPU available, running on CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import ReadInstruction\n",
    "\n",
    "# Enable GPU usage, if we can.\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Enabling GPU usage\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(device)\n",
    "    IS_GPU = True\n",
    "else:\n",
    "    print(\"No GPU available, running on CPU\")\n",
    "    device = torch.device(\"cpu\") # Note: macOS incompatible with NVIDIA GPUs\n",
    "    IS_GPU = False\n",
    "    \n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDH Dataset Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data loaded from disk.\nEvaluation data loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "from src.LDHData import LDHData\n",
    "\n",
    "data = LDHData()\n",
    "data.load_training_data()\n",
    "data.load_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-c09023214f11a34f.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRETRAINED_MODEL_NAME)  \n",
    "# Wrap tokenizer.\n",
    "def tokenize(x):\n",
    "    tokenized = tokenizer(x, add_special_tokens=True, padding=\"max_length\", max_length=150)\n",
    "    return tokenized\n",
    "    \n",
    "encoded_train = data.train_dataset['far'].map(\n",
    "    lambda ds: tokenize(ds['response']), batched=True, batch_size=16\n",
    ")\n",
    "encoded_train.set_format(type='torch', output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_train = encoded_train.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, TrainingArguments, Trainer\n",
    "from src.ReappModel import ReappModel\n",
    "# Define the parameters under which the model will be trained.\n",
    "# By default, uses an AdamW optimizer w/ linear warmup.\n",
    "model = ReappModel(DistilBertModel.from_pretrained(PRETRAINED_MODEL_NAME))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-b00d733959a806b5.arrow\n",
      "Loading cached split indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-727b26d67b867b76.arrow and /Users/danielpham/Documents/code/reapp/src/training/far/cache-22a98a69e1066163.arrow\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.85s/it]\n",
      "Loading cached shuffled indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-b00d733959a806b5.arrow\n",
      "Loading cached split indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-727b26d67b867b76.arrow and /Users/danielpham/Documents/code/reapp/src/training/far/cache-22a98a69e1066163.arrow\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]{'train_runtime': 4.8515, 'train_samples_per_second': 0.206, 'epoch': 1.0}\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.96s/it]\n",
      "Loading cached shuffled indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-b00d733959a806b5.arrow\n",
      "Loading cached split indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-727b26d67b867b76.arrow and /Users/danielpham/Documents/code/reapp/src/training/far/cache-22a98a69e1066163.arrow\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]{'train_runtime': 4.9803, 'train_samples_per_second': 0.201, 'epoch': 1.0}\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.80s/it]\n",
      "Loading cached shuffled indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-b00d733959a806b5.arrow\n",
      "Loading cached split indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-727b26d67b867b76.arrow and /Users/danielpham/Documents/code/reapp/src/training/far/cache-22a98a69e1066163.arrow\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]{'train_runtime': 4.8246, 'train_samples_per_second': 0.207, 'epoch': 1.0}\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.79s/it]\n",
      "Loading cached shuffled indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-b00d733959a806b5.arrow\n",
      "Loading cached split indices for dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-727b26d67b867b76.arrow and /Users/danielpham/Documents/code/reapp/src/training/far/cache-22a98a69e1066163.arrow\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]{'train_runtime': 4.8103, 'train_samples_per_second': 0.208, 'epoch': 1.0}\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.76s/it]{'train_runtime': 4.7821, 'train_samples_per_second': 0.209, 'epoch': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained = []\n",
    "for i in range(5):\n",
    "    encoded_train_split, encoded_val_split = encoded_train.shuffle().train_test_split(test_size=0.1).values()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,                  \n",
    "        train_dataset=encoded_train_split,\n",
    "        eval_dataset=encoded_val_split         \n",
    "    )\n",
    "    output = trainer.train()\n",
    "    trained.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(<transformers.trainer.Trainer at 0x143f42e20>,\n",
       "  TrainOutput(global_step=1, training_loss=19.225055694580078, metrics={'train_runtime': 4.8515, 'train_samples_per_second': 0.206, 'epoch': 1.0})),\n",
       " (<transformers.trainer.Trainer at 0x143f4d8e0>,\n",
       "  TrainOutput(global_step=1, training_loss=18.495969772338867, metrics={'train_runtime': 4.9803, 'train_samples_per_second': 0.201, 'epoch': 1.0})),\n",
       " (<transformers.trainer.Trainer at 0x143f79550>,\n",
       "  TrainOutput(global_step=1, training_loss=17.777496337890625, metrics={'train_runtime': 4.8246, 'train_samples_per_second': 0.207, 'epoch': 1.0})),\n",
       " (<transformers.trainer.Trainer at 0x13ff65580>,\n",
       "  TrainOutput(global_step=1, training_loss=17.065771102905273, metrics={'train_runtime': 4.8103, 'train_samples_per_second': 0.208, 'epoch': 1.0})),\n",
       " (<transformers.trainer.Trainer at 0x143eb6490>,\n",
       "  TrainOutput(global_step=1, training_loss=16.521961212158203, metrics={'train_runtime': 4.7821, 'train_samples_per_second': 0.209, 'epoch': 1.0}))]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}