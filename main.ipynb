{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training For Linguistic Distancing and Emotion Regulation\n",
    "\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "> pipenv shell  #Generates a new virtual environment based on Pipfile\n",
    "> pipenv install # Installs the packages in Pipfile.lock (Use --dev) to also install dev packages\n",
    "```\n",
    "## Included Datasets\n",
    "- LDHII \n",
    "- \n",
    "- Emobank\n",
    "**Sources**\n",
    "-  [Sentiment Analysis Text Classification Tutorial](https://www.youtube.com/watch?v=8N-nM3QW7O0)\n",
    "- [Using Catalyst for Training Organization](https://github.com/catalyst-team/catalyst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reinitialized existing Git repository in /Users/danielpham/Documents/code/reapp/.git/\n",
      "fatal: remote origin already exists.\n",
      "M\tpoetry.lock\n",
      "M\tpyproject.toml\n",
      "M\tsrc/LDHData.py\n",
      "M\tsrc/ReappModel.py\n",
      "Already on 'dev'\n",
      "Your branch is ahead of 'origin/dev' by 2 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "This repository is configured for Git LFS but 'git-lfs' was not found on your path. If you no longer wish to use Git LFS, remove this hook by deleting .git/hooks/post-checkout.\n",
      "\n",
      "Requirement already satisfied: transformers in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (4.2.2)\n",
      "Requirement already satisfied: datasets in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (1.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (1.18.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: xxhash in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: pandas in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (1.2.1)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: dill in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: multiprocess in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: filelock in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: packaging in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from pandas->datasets) (2020.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: joblib in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/danielpham/Library/Caches/pypoetry/virtualenvs/reapp-p6yGuJKm-py3.8/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Uncomment to add the src code to the working dir when on colab\n",
    "# git init\n",
    "# git config core.sparseCheckout true\n",
    "# git remote add -f origin https://github.com/danielcpham/reappraisal-model.git\n",
    "# echo \"src/\" > .git/info/sparse-checkout\n",
    "# echo \"poetry.lock\" >> .git/info/sparse-checkout\n",
    "#echo \"pyproject.toml\" >> .git/info/sparse-checkout \n",
    "# git checkout dev\n",
    "\n",
    "# ! pip install transformers datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No GPU available, running on CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import ReadInstruction\n",
    "\n",
    "# Enable GPU usage, if we can.\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Enabling GPU usage\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(device)\n",
    "    IS_GPU = True\n",
    "else:\n",
    "    print(\"No GPU available, running on CPU\")\n",
    "    device = torch.device(\"cpu\") # Note: macOS incompatible with NVIDIA GPUs\n",
    "    IS_GPU = False\n",
    "    \n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDH Dataset Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data loaded from disk.\nEvaluation data loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "from src.LDHData import LDHData\n",
    "\n",
    "data = LDHData()\n",
    "data.load_training_data()\n",
    "data.load_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /Users/danielpham/Documents/code/reapp/src/training/far/cache-c09023214f11a34f.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRETRAINED_MODEL_NAME)  \n",
    "# Wrap tokenizer.\n",
    "def tokenize(x):\n",
    "    tokenized = tokenizer(x, add_special_tokens=True, padding=\"max_length\", max_length=150)\n",
    "    return tokenized\n",
    "    \n",
    "encoded_train = data.train_dataset['far'].map(\n",
    "    lambda ds: tokenize(ds['response']), batched=True, batch_size=16\n",
    ")\n",
    "encoded_train.set_format(type='torch', output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0052,  0.1478],\n        [-0.2810,  0.2687],\n        [ 0.0080,  0.1550],\n        [-0.0645, -0.0440],\n        [-0.3149,  0.3754],\n        [-0.0875,  0.0917],\n        [-0.2683,  0.2093],\n        [-0.1531,  0.2634],\n        [-0.2345,  0.2897],\n        [-0.1325,  0.2053],\n        [-0.2943,  0.1872],\n        [ 0.0488,  0.0393],\n        [-0.1339, -0.0532],\n        [-0.1767,  0.2387],\n        [ 0.0352, -0.0639],\n        [-0.0308,  0.1296]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(17.3436)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from transformers import DistilBertModel, TrainingArguments, Trainer\n",
    "from src.ReappModel import ReappModel\n",
    "# Define the parameters under which the model will be trained.\n",
    "# By default, uses an AdamW optimizer w/ linear warmup.\n",
    "model = ReappModel(DistilBertModel.from_pretrained(PRETRAINED_MODEL_NAME))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,                  \n",
    "    train_dataset=encoded_train,\n",
    "    #eval_dataset=encoded_eval         \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}