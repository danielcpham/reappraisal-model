{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ldhdata\n",
    "# export\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LDHData:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"Initializes the directories from which the data will be coming and leaving.\"\"\"\n",
    "        input_dir = Path(data_dir, \"data\")\n",
    "        output_dir = Path(data_dir, \"output\")\n",
    "        self.train_dir = (\n",
    "            input_dir / \"training\"\n",
    "        )  # Where the training data is coming from\n",
    "        self.eval_dir = input_dir / \"eval\"  # Test data\n",
    "        # TODO: assert that the directories are valid.\n",
    "        self.datasets = defaultdict(str)\n",
    "        self.save_dir = output_dir  # Where cached data is saved.\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.datasets[\"train\"]\n",
    "\n",
    "    @property\n",
    "    def eval_dataset(self):\n",
    "        return self.datasets[\"eval\"]\n",
    "\n",
    "    def load_training_data(self, force_reload=False, save_datasets=True) -> None:\n",
    "        training_save_dir = self.save_dir / \"training\"\n",
    "        try:\n",
    "            if force_reload:\n",
    "                raise Exception()\n",
    "            # If the training data has already been save, load it from the save_directory\n",
    "            self.datasets[\"train\"] = DatasetDict.load_from_disk(training_save_dir)\n",
    "            print(\"Training data loaded from disk.\")\n",
    "        except:\n",
    "            # If it hasn't regenerate the training data.\n",
    "            print(\"Regenerating training data.\")\n",
    "            train_df_dict = self._parse_training_data(self.train_dir)\n",
    "            self.datasets[\"train\"] = DatasetDict(\n",
    "                {\n",
    "                    \"far\": Dataset.from_pandas(train_df_dict[\"far\"]),\n",
    "                    \"obj\": Dataset.from_pandas(train_df_dict[\"obj\"]),\n",
    "                }\n",
    "            )\n",
    "            if save_datasets:\n",
    "                print(f\"Saving training dataset to {training_save_dir}\")\n",
    "                self.datasets[\"train\"].save_to_disk(training_save_dir)\n",
    "\n",
    "    def load_eval_data(self, force_reload=False, save_datasets=True) -> None:\n",
    "        eval_save_dir = self.save_dir / \"eval\"\n",
    "        try:\n",
    "            if force_reload:\n",
    "                raise Exception()\n",
    "            self.datasets[\"eval\"] = DatasetDict.load_from_disk(eval_save_dir)\n",
    "            print(\"Evaluation data loaded from disk.\")\n",
    "        except:\n",
    "            print(\"Regenerating evaluation data.\")\n",
    "            eval_df_dict = self._parse_eval_data(self.eval_dir)\n",
    "            self.datasets[\"eval\"] = DatasetDict(\n",
    "                {\n",
    "                    \"far\": Dataset.from_pandas(eval_df_dict[\"far\"]),\n",
    "                    \"obj\": Dataset.from_pandas(eval_df_dict[\"obj\"]),\n",
    "                }\n",
    "            )\n",
    "            if save_datasets:\n",
    "                print(f\"Saving evaluation dataset to {eval_save_dir}\")\n",
    "                self.datasets[\"eval\"].save_to_disk(eval_save_dir)\n",
    "\n",
    "    def collapse_eval_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Let df be the dataframe obtained from loading evaluation data.\n",
    "        Expand the text in 'response' to have a single sentence per response.\n",
    "        source: https://medium.com/@johnadungan/expanding-lists-in-panda-dataframes-2724803498f8\n",
    "        \"\"\"\n",
    "        df[\"response\"] = df[\"response\"].map(sent_tokenize, na_action=\"ignore\")\n",
    "        texts = df[\"response\"].dropna()\n",
    "        lens_of_lists = texts.apply(len)\n",
    "        origin_rows = range(texts.shape[0])\n",
    "        destination_rows = np.repeat(origin_rows, lens_of_lists)\n",
    "        non_list_cols = [idx for idx, col in enumerate(df.columns) if col != \"response\"]\n",
    "        expanded_df = df.iloc[destination_rows, non_list_cols].copy()\n",
    "        expanded_df[\"split_response\"] = [i for items in texts for i in items]\n",
    "        expanded_df = expanded_df[expanded_df[\"split_response\"] != \".\"].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        assert expanded_df.apply(pd.unique)[\"daycode\"].size == 5\n",
    "        assert expanded_df.apply(pd.unique)[\"Condition\"].size == 3\n",
    "        expanded_df.rename(columns={\"split_response\": \"response\"}, inplace=True)\n",
    "        return expanded_df\n",
    "\n",
    "    # Functions to read the data files directly\n",
    "    def _parse_training_data(self, train_data_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        study1 = pd.read_csv(Path(train_data_dir, \"Master_Final_TrainingData.csv\", usecols = ['Text Response', \"AVG_OBJ\", \"AVG_FAR\"]))\n",
    "\n",
    "        study1far = study1.rename(columns={\n",
    "            'Text Response': 'response',\n",
    "            'AVG_FAR': 'score'\n",
    "        }).drop(columns='AVG_OBJ')\n",
    "\n",
    "        study1obj = study1.rename(columns={\n",
    "            'Text Response': 'response',\n",
    "            'AVG_OBJ': 'score'\n",
    "        }).drop(columns='AVG_FAR')\n",
    "\n",
    "        return {\n",
    "            'far': study1far,\n",
    "            'obj': study1obj\n",
    "        }\n",
    "\n",
    "    def _parse_eval_data(self, eval_data_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\" Parses evaluation data. \n",
    "\n",
    "        Args:\n",
    "            eval_dir (str): The folder in which original training data is stored.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: [description]\n",
    "        \"\"\"\n",
    "        # Read the excel files\n",
    "        eval_far_data = pd.read_excel(\n",
    "            os.path.join(eval_data_dir, \"Alg_Far_NEW.xlsx\"), engine=\"openpyxl\"\n",
    "        ).rename(columns={\"TextResponse\": \"response\"})\n",
    "        eval_obj_data = pd.read_excel(\n",
    "            os.path.join(eval_data_dir, \"Alg_Obj_NEW.xlsx\"), engine=\"openpyxl\"\n",
    "        ).rename(columns={\"TextResponse\": \"response\"})\n",
    "        eval_far_data = eval_far_data[eval_far_data[\"response\"].notna()]\n",
    "        eval_obj_data = eval_obj_data[eval_obj_data[\"response\"].notna()]\n",
    "        return {\n",
    "            \"far\": self.collapse_eval_data(eval_far_data),\n",
    "            \"obj\": self.collapse_eval_data(eval_obj_data),\n",
    "        }\n",
    "\n",
    "\n",
    "#\n",
    "def _expand_response(input_response: str) -> List[str]:\n",
    "    sentences = sent_tokenize(input_response)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "DEFAULT_TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\", use_fast=True\n",
    ")\n",
    "\n",
    "class LDHDataModule(lit.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        batch_size=16,\n",
    "        tokenizer=DEFAULT_TOKENIZER,\n",
    "        strat=\"obj\",\n",
    "        kfolds=5,\n",
    "        force_reload=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.strat = strat\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kfolds = kfolds\n",
    "        self.current_split = 0\n",
    "        data = LDHData(data_dir)\n",
    "\n",
    "        self.train_data = self.load_training_data(data, force_reload)\n",
    "        self.indices = (\n",
    "            self.assign_groups()\n",
    "        )  # Get an array of groups assigned for each data point\n",
    "        self.splits = self.generate_splits(\n",
    "            self.indices\n",
    "        )  # Use GroupKFold to generate specific splits\n",
    "        self.eval_data = self.load_eval_data(data, force_reload)\n",
    "\n",
    "    def load_training_data(self, data: LDHData, force_reload):\n",
    "        data.load_training_data(force_reload)\n",
    "        train_data: Dataset = data.train_dataset[self.strat]\n",
    "        print(\"Encoding Train Data:\")\n",
    "        encoded_ds = train_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"score\"],\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        return encoded_ds\n",
    "\n",
    "    def load_eval_data(self, data: LDHData, force_reload):\n",
    "        data.load_eval_data(force_reload)\n",
    "        eval_data: Dataset = data.eval_dataset[self.strat]\n",
    "        print(\"Encoding Test Data:\")\n",
    "        encoded_ds = eval_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        # Need to include addcode here as well\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\"],\n",
    "            output_all_columns=True,\n",
    "        )\n",
    "        return encoded_ds\n",
    "\n",
    "    def assign_groups(self):\n",
    "        \"\"\"Assign each datapoint to a group for later kfold validation\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.train_data))\n",
    "        indices = indices % self.kfolds\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def generate_splits(self, indices):\n",
    "        cv = GroupKFold(self.kfolds)\n",
    "        splits = list(cv.split(self.train_data, groups=indices))\n",
    "        return splits\n",
    "\n",
    "    def get_train_dataloader(self, split: int):\n",
    "        train_idx = self.splits[split][\n",
    "            0\n",
    "        ].tolist()  # retrieve the split generated by GroupKFold\n",
    "        data = Subset(self.train_data, train_idx)\n",
    "        return DataLoader(data, batch_size=self.batch_size)\n",
    "\n",
    "    def get_val_dataloader(self, split: int):\n",
    "        val_idx = self.splits[split][\n",
    "            1\n",
    "        ].tolist()  # retrieve the split generated by GroupKFold\n",
    "        data = Subset(self.train_data, val_idx)\n",
    "        return DataLoader(data, batch_size=self.batch_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_train_dataloader(self.current_split)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_val_dataloader(self.current_split)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.eval_data, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script(\"LDHData.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
