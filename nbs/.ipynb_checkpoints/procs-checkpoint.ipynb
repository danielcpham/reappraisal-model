{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- When running on Google Colab, mount Google Drive to access scripts.\n",
    "- `cd` into the project root and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/Users/danielpham/Google Drive/ldh\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#%cd \"/content/drive/MyDrive/ldh\"\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "%cd \"/Users/danielpham/Google Drive/ldh\"\n",
    "# %pip install transformers datasets pytorch-lightning nltk matplotlib \"ray[tune]\"\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/danielpham/Google Drive/ldh/output/training/far/cache-22098b089e6d8812.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded from disk.\n",
      "Encoding Train Data:\n",
      "Evaluation data loaded from disk.\n",
      "Encoding Test Data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1b1780d9b54572ad38abcfccccfee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32109.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/danielpham/Google Drive/ldh')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd \n",
    "import pytorch_lightning as lit \n",
    "\n",
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "\n",
    "num_folds = 3\n",
    "batch_size = 16\n",
    "strat = 'far'\n",
    "\n",
    "ldhdata = LDHDataModule(batch_size=batch_size, strat=strat, kfolds=num_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/danielpham/Google Drive/ldh')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EarlyStopping mode set to min for monitoring val_loss.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "#from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "#from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "\n",
    "\n",
    "default_config = {\n",
    "    'lr': 1e-3,\n",
    "    'hidden_layer_size': 50\n",
    "}\n",
    "\n",
    "\n",
    "save_dir = Path.cwd() / 'output'\n",
    "logger = lit.loggers.TensorBoardLogger(save_dir=save_dir)\n",
    "\n",
    "# Stops when the val loss stops changing\n",
    "callback_earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=True)\n",
    "# Saves the top 3 checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory /Users/danielpham/Google Drive/ldh/output exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 2 batch(es).\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | model      | DistilBertModel  | 66.4 M\n",
      "1 | avg        | AvgPool1d        | 0     \n",
      "2 | classifier | Sequential       | 629 K \n",
      "3 | train_loss | MeanSquaredError | 0     \n",
      "4 | val_loss   | MeanSquaredError | 0     \n",
      "------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on split 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216b30f000d44152886c4102074e94a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(4.3218)}, {'loss': tensor(3.3146)}]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 2 batch(es).\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | model      | DistilBertModel  | 66.4 M\n",
      "1 | avg        | AvgPool1d        | 0     \n",
      "2 | classifier | Sequential       | 629 K \n",
      "3 | train_loss | MeanSquaredError | 0     \n",
      "4 | val_loss   | MeanSquaredError | 0     \n",
      "------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ece9a45d484ff089ac370dfe02bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(18.8731)}, {'loss': tensor(18.0243)}]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 2 batch(es).\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | model      | DistilBertModel  | 66.4 M\n",
      "1 | avg        | AvgPool1d        | 0     \n",
      "2 | classifier | Sequential       | 629 K \n",
      "3 | train_loss | MeanSquaredError | 0     \n",
      "4 | val_loss   | MeanSquaredError | 0     \n",
      "------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55524f692fc14fed817bf53e8995cb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(12.3200)}, {'loss': tensor(20.2766)}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### K-FOLD CV\n",
    "%autoreload 2 \n",
    "from datetime import datetime\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "\n",
    "for i in range(num_folds):\n",
    "    split = i # Current split being trained\n",
    "    train_dl = ldhdata.get_train_dataloader(split)\n",
    "    val_dl = ldhdata.get_val_dataloader(split)\n",
    "    \n",
    "    strat = ldhdata.strat\n",
    "    dt = datetime.now() # get the datetime \n",
    "    \n",
    "    callback_checkpoint = ModelCheckpoint(\n",
    "        dirpath=save_dir,\n",
    "        filename='{epoch}-{val_loss:.2f}',\n",
    "        monitor='val_loss', verbose=False, \n",
    "        save_last=False, save_top_k=1, save_weights_only=False, \n",
    "        mode='min', period=1, prefix=strat)\n",
    "    \n",
    "    model = LightningReapp(default_config)\n",
    "    trainer = lit.Trainer(\n",
    "        fast_dev_run=2,\n",
    "        logger=logger,\n",
    "        max_epochs=50,\n",
    "        gradient_clip_val=1.0,\n",
    "        progress_bar_refresh_rate=30,\n",
    "        terminate_on_nan=True,\n",
    "        weights_summary=\"top\",\n",
    "        weights_save_path=save_dir,\n",
    "        callbacks=[callback_checkpoint,callback_early_stopping])\n",
    "    print(f\"Training on split {i + 1}\")\n",
    "    trainer.fit(model, train_dl, val_dl)\n",
    "    print(callback_checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
