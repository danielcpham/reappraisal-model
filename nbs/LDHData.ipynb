{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ldhdata\n",
    "# export\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LDHData:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"Initializes the directories from which the data will be coming and leaving.\"\"\"\n",
    "        input_dir = Path(data_dir, \"data\")\n",
    "        output_dir = Path(data_dir, \"output\")\n",
    "        self.train_dir = (\n",
    "            input_dir / \"training\"\n",
    "        )  # Where the training data is coming from\n",
    "        self.eval_dir = input_dir / \"eval\"  # Test data\n",
    "        # TODO: assert that the directories are valid.\n",
    "        self.datasets = defaultdict(str)\n",
    "        self.save_dir = output_dir  # Where cached data is saved.\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.datasets[\"train\"]\n",
    "\n",
    "    @property\n",
    "    def eval_dataset(self):\n",
    "        return self.datasets[\"eval\"]\n",
    "\n",
    "    def load_training_data(self, force_reload=False, save_datasets=True) -> None:\n",
    "        training_save_dir = self.save_dir / \"training\"\n",
    "        try:\n",
    "            if force_reload:\n",
    "                raise Exception()\n",
    "            # If the training data has already been save, load it from the save_directory\n",
    "            self.datasets[\"train\"] = DatasetDict.load_from_disk(training_save_dir)\n",
    "            print(\"Training data loaded from disk.\")\n",
    "        except:\n",
    "            # If it hasn't regenerate the training data.\n",
    "            print(\"Regenerating training data.\")\n",
    "            train_df_dict = self._parse_training_data(self.train_dir)\n",
    "            self.datasets[\"train\"] = DatasetDict(\n",
    "                {\n",
    "                    \"far\": Dataset.from_pandas(train_df_dict[\"far\"]),\n",
    "                    \"obj\": Dataset.from_pandas(train_df_dict[\"obj\"]),\n",
    "                }\n",
    "            )\n",
    "            if save_datasets:\n",
    "                print(f\"Saving training dataset to {training_save_dir}\")\n",
    "                self.datasets[\"train\"].save_to_disk(training_save_dir)\n",
    "\n",
    "    def load_eval_data(self, force_reload=False, save_datasets=True) -> None:\n",
    "        eval_save_dir = self.save_dir / \"eval\"\n",
    "        try:\n",
    "            if force_reload:\n",
    "                raise Exception()\n",
    "            self.datasets[\"eval\"] = DatasetDict.load_from_disk(eval_save_dir)\n",
    "            print(\"Evaluation data loaded from disk.\")\n",
    "        except:\n",
    "            print(\"Regenerating evaluation data.\")\n",
    "            eval_df_dict = self._parse_eval_data(self.eval_dir)\n",
    "            self.datasets[\"eval\"] = DatasetDict(\n",
    "                {\n",
    "                    \"far\": Dataset.from_pandas(eval_df_dict[\"far\"]),\n",
    "                    \"obj\": Dataset.from_pandas(eval_df_dict[\"obj\"]),\n",
    "                }\n",
    "            )\n",
    "            if save_datasets:\n",
    "                print(f\"Saving evaluation dataset to {eval_save_dir}\")\n",
    "                self.datasets[\"eval\"].save_to_disk(eval_save_dir)\n",
    "\n",
    "    def collapse_eval_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Let df be the dataframe obtained from loading evaluation data.\n",
    "        Expand the text in 'response' to have a single sentence per response.\n",
    "        source: https://medium.com/@johnadungan/expanding-lists-in-panda-dataframes-2724803498f8\n",
    "        \"\"\"\n",
    "        df[\"response\"] = df[\"response\"].map(sent_tokenize, na_action=\"ignore\")\n",
    "        texts = df[\"response\"].dropna()\n",
    "        lens_of_lists = texts.apply(len)\n",
    "        origin_rows = range(texts.shape[0])\n",
    "        destination_rows = np.repeat(origin_rows, lens_of_lists)\n",
    "        non_list_cols = [idx for idx, col in enumerate(df.columns) if col != \"response\"]\n",
    "        expanded_df = df.iloc[destination_rows, non_list_cols].copy()\n",
    "        expanded_df[\"split_response\"] = [i for items in texts for i in items]\n",
    "        expanded_df = expanded_df[expanded_df[\"split_response\"] != \".\"].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        expanded_df.rename(columns={\"split_response\": \"response\"}, inplace=True)\n",
    "        return expanded_df\n",
    "\n",
    "    # Functions to read the data files directly\n",
    "    def _parse_training_data(self, train_data_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        study1 = pd.read_csv(Path(train_data_dir, \"Master_Final_TrainingData.csv\"), usecols = ['Text Response', \"AVG_OBJ\", \"AVG_FAR\"])\n",
    "        study1far = study1.rename(columns={\n",
    "            'Text Response': 'response',\n",
    "            'AVG_FAR': 'score'\n",
    "        }).drop(columns='AVG_OBJ')\n",
    "\n",
    "        study1obj = study1.rename(columns={\n",
    "            'Text Response': 'response',\n",
    "            'AVG_OBJ': 'score'\n",
    "        }).drop(columns='AVG_FAR')\n",
    "\n",
    "\n",
    "        return {\n",
    "            'far': study1far,\n",
    "            'obj': study1obj\n",
    "        }\n",
    "\n",
    "    def _parse_eval_data(self, eval_data_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\" Parses evaluation data.\n",
    "\n",
    "        Args:\n",
    "            eval_dir (str): The folder in which original training data is stored.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: [description]\n",
    "        \"\"\"\n",
    "        # Read the excel files\n",
    "        eval_far_data = pd.read_excel(\n",
    "            os.path.join(eval_data_dir, \"Alg_Far_NEW.xlsx\"), engine=\"openpyxl\"\n",
    "        ).rename(columns={\"TextResponse\": \"response\"})\n",
    "        eval_obj_data = pd.read_excel(\n",
    "            os.path.join(eval_data_dir, \"Alg_Obj_NEW.xlsx\"), engine=\"openpyxl\"\n",
    "        ).rename(columns={\"TextResponse\": \"response\"})\n",
    "        eval_far_data = eval_far_data[eval_far_data[\"response\"].notna()]\n",
    "        eval_obj_data = eval_obj_data[eval_obj_data[\"response\"].notna()]\n",
    "        return {\n",
    "            \"far\": self.collapse_eval_data(eval_far_data),\n",
    "            \"obj\": self.collapse_eval_data(eval_obj_data),\n",
    "        }\n",
    "\n",
    "\n",
    "def _expand_response(input_response: str) -> List[str]:\n",
    "    sentences = sent_tokenize(input_response)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae1c961f906b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m DEFAULT_TOKENIZER = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"distilbert-base-uncased-finetuned-sst-2-english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# export\n",
    "DEFAULT_TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\", use_fast=True\n",
    ")\n",
    "\n",
    "class LDHDataModule(lit.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        batch_size=16,\n",
    "        tokenizer=DEFAULT_TOKENIZER,\n",
    "        strat=\"obj\",\n",
    "        kfolds=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data = LDHData(data_dir)\n",
    "        self.strat = strat\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kfolds = kfolds\n",
    "        self.current_split = 0\n",
    "        self.train_data = None\n",
    "        self.eval_data = None\n",
    "        self.splits = []\n",
    "\n",
    "        # self.load_training_data(data, force_reload)\n",
    "        # self.load_eval_data(data, force_reload)\n",
    "        # self.splits = self.generate_splits(self.kfolds)\n",
    "\n",
    "    def get_train_data(self, encoded=False):\n",
    "        if not self.train_data:\n",
    "            raise Exception(\"Please load the training data first!\")\n",
    "\n",
    "        train_data = self.train_data[self.strat]\n",
    "        if not encoded:\n",
    "            return train_data\n",
    "        print(\"Encoding Training Data:\")\n",
    "        encoded_ds = train_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"score\"],\n",
    "        )\n",
    "        return encoded_ds\n",
    "\n",
    "    def load_train_data(self, force_reload=False):\n",
    "        self.data.load_training_data(force_reload)\n",
    "        train_data: Dataset = self.data.train_dataset[self.strat]\n",
    "        print(\"Encoding Training Data:\")\n",
    "        encoded_ds = train_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"score\"],\n",
    "        )\n",
    "        self.train_data = encoded_ds\n",
    "        self.splits = self.generate_splits(self.kfolds)\n",
    "\n",
    "\n",
    "    def load_eval_data(self, force_reload=False):\n",
    "        self.data.load_eval_data(force_reload)\n",
    "        eval_data: Dataset = self.data.eval_dataset[self.strat]\n",
    "        print(\"Encoding Test Data\")\n",
    "        encoded_ds = eval_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        # Need to include addcode here as well\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\"],\n",
    "        )\n",
    "        self.eval_data = encoded_ds\n",
    "\n",
    "    def generate_splits(self, num_groups=1)-> List[Tuple[int, int]]:\n",
    "        \"\"\"Generates splits of training data.\n",
    "        Args:\n",
    "            num_groups (int, optional): [description]. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[int, int]]: [description]\n",
    "        \"\"\"\n",
    "        train_data = self.train_data\n",
    "        indices = np.arange(len(train_data))\n",
    "        if num_groups == 1:\n",
    "            return [train_test_split(indices, train_size=0.1)]\n",
    "        indices = indices % num_groups\n",
    "        np.random.shuffle(indices)\n",
    "        cv = GroupKFold(num_groups)\n",
    "        splits = list(cv.split(train_data, groups=indices))\n",
    "        return splits\n",
    "\n",
    "    def get_train_dataloader(self, split: int):\n",
    "        train_idx = self.splits[split][0].tolist()  # retrieve the split generated by GroupKFold\n",
    "        data = Subset(self.train_data, train_idx)\n",
    "        return DataLoader(data, batch_size=self.batch_size)\n",
    "\n",
    "    def get_val_dataloader(self, split: int):\n",
    "        val_idx = self.splits[split][1].tolist()  # retrieve the split generated by GroupKFold\n",
    "        data = Subset(self.train_data, val_idx)\n",
    "        return DataLoader(data, batch_size=self.batch_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_train_dataloader(self.current_split)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_val_dataloader(self.current_split)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.eval_data, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LDHData.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script(\"LDHData.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded from disk.\n",
      "Evaluation data loaded from disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Bin', 'Condition', 'Cue1', 'Cue2', 'Group', 'Group_ID', 'Rating', 'ReappSuccess', 'Subj_ID', 'addcode', 'daycode', 'observed', 'response'],\n",
       "    num_rows: 32109\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = LDHDataModule(data_dir='/Users/danielpham/Google Drive/ldh/')\n",
    "data.load_train_data()\n",
    "data.load_eval_data()\n",
    "\n",
    "data.get_train_data()\n",
    "data.get_eval_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "metadata": {
    "interpreter": {
     "hash": "6cedeabb5f601728273af76a0df97b854fca999a9667201438ccd991d5fa9a34"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
