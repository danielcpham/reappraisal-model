{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp ldhdata\n",
    "#export\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class LDHData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        data_dir\n",
    "    ):\n",
    "        \"\"\"Initializes the directories from which the data will be coming and leaving.\n",
    "        \"\"\"\n",
    "        input_dir =  Path( data_dir, 'data')\n",
    "        output_dir = Path( data_dir, 'output')\n",
    "        self.train_dir = (\n",
    "            input_dir / \"training\"\n",
    "        )  # Where the training data is coming from\n",
    "        self.eval_dir = input_dir / \"eval\"  # Test data\n",
    "        # TODO: assert that the directories are valid.\n",
    "        self.datasets = defaultdict(str)\n",
    "        self.save_dir = output_dir  # Where cached data is saved.\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.datasets[\"train\"]\n",
    "\n",
    "    @property\n",
    "    def eval_dataset(self):\n",
    "        return self.datasets[\"eval\"]\n",
    "\n",
    "    def load_training_data(\n",
    "        self, force_reload=False, save_datasets=True\n",
    "    ) -> None:\n",
    "        training_save_dir = self.save_dir / \"training\"\n",
    "        try:\n",
    "            if force_reload:\n",
    "                raise Exception()\n",
    "            # If the training data has already been save, load it from the save_directory\n",
    "            self.datasets[\"train\"] = DatasetDict.load_from_disk(training_save_dir)\n",
    "            print(\"Training data loaded from disk.\")\n",
    "        except:\n",
    "            # If it hasn't regenerate the training data.\n",
    "            print(\"Regenerating training data.\")\n",
    "            train_df_dict = self._parse_training_data(self.train_dir)\n",
    "            self.datasets[\"train\"] = DatasetDict(\n",
    "                {\n",
    "                    \"far\": Dataset.from_pandas(train_df_dict[\"far\"]),\n",
    "                    \"obj\": Dataset.from_pandas(train_df_dict[\"obj\"]),\n",
    "                }\n",
    "            )\n",
    "            if save_datasets:\n",
    "                print(f\"Saving training dataset to {training_save_dir}\")\n",
    "                self.datasets[\"train\"].save_to_disk(training_save_dir)\n",
    "\n",
    "    def load_eval_data(self, force_reload=False, save_datasets=True) -> None:\n",
    "        eval_save_dir = self.save_dir / \"eval\"\n",
    "        try:\n",
    "            if force_reload:\n",
    "                raise Exception()\n",
    "            self.datasets[\"eval\"] = DatasetDict.load_from_disk(eval_save_dir)\n",
    "            print(\"Evaluation data loaded from disk.\")\n",
    "        except:\n",
    "            print(\"Regenerating evaluation data.\")\n",
    "            eval_df_dict = self._parse_eval_data(self.eval_dir)\n",
    "            self.datasets[\"eval\"] = DatasetDict(\n",
    "                {\n",
    "                    \"far\": Dataset.from_pandas(eval_df_dict[\"far\"]),\n",
    "                    \"obj\": Dataset.from_pandas(eval_df_dict[\"obj\"]),\n",
    "                }\n",
    "            )\n",
    "            if save_datasets:\n",
    "                print(f\"Saving evaluation dataset to {eval_save_dir}\")\n",
    "                self.datasets[\"eval\"].save_to_disk(eval_save_dir)\n",
    "\n",
    "    def collapse_eval_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Let df be the dataframe obtained from loading evaluation data.\n",
    "        Expand the text in 'response' to have a single sentence per response.\n",
    "        source: https://medium.com/@johnadungan/expanding-lists-in-panda-dataframes-2724803498f8\n",
    "        \"\"\"\n",
    "        df['response'] = df['response'].map(sent_tokenize, na_action='ignore')\n",
    "        texts = df['response'].dropna()\n",
    "        lens_of_lists = texts.apply(len)\n",
    "        origin_rows = range(texts.shape[0])\n",
    "        destination_rows = np.repeat(origin_rows, lens_of_lists)\n",
    "        non_list_cols = [idx for idx, col in enumerate(df.columns) \n",
    "                        if col != 'response']\n",
    "        expanded_df = df.iloc[destination_rows, non_list_cols].copy()\n",
    "        expanded_df['split_response'] = [i for items in texts\n",
    "                                    for i in items]\n",
    "        expanded_df = expanded_df[expanded_df['split_response'] != \".\"].reset_index(drop=True)\n",
    "        assert expanded_df.apply(pd.unique)['daycode'].size == 5\n",
    "        assert expanded_df.apply(pd.unique)['Condition'].size == 3\n",
    "        expanded_df.rename(columns={'split_response': 'response'}, inplace=True)\n",
    "        return expanded_df\n",
    "\n",
    "    # Functions to read the data files directly\n",
    "    def _parse_training_data(self, train_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        files = os.listdir(train_dir)\n",
    "        dfs: List[pd.DataFrame] = []\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                filename = os.path.join(train_dir, file)\n",
    "                dfs.append(\n",
    "                    pd.read_csv(\n",
    "                        filename, header=0, names=[\"response\", \"spatiotemp\", \"obj\"]\n",
    "                    )\n",
    "                )\n",
    "        ldh: pd.DataFrame = pd.concat(dfs, ignore_index=True)\n",
    "        train_far_data = ldh[[\"response\", \"spatiotemp\"]].rename(\n",
    "            columns={\"spatiotemp\": \"score\"}\n",
    "        )\n",
    "        train_obj_data = ldh[[\"response\", \"obj\"]].rename(columns={\"obj\": \"score\"})\n",
    "        return {\"far\": train_far_data, \"obj\": train_obj_data}\n",
    "\n",
    "    def _parse_eval_data(self, eval_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        # Read the excel files\n",
    "        eval_far_data = pd.read_excel(\n",
    "            os.path.join(self.eval_dir, \"Alg_Far_NEW.xlsx\"),\n",
    "            engine=\"openpyxl\"\n",
    "        ).rename(columns={\"TextResponse\": \"response\"})\n",
    "        eval_obj_data = pd.read_excel(\n",
    "            os.path.join(self.eval_dir, \"Alg_Obj_NEW.xlsx\"),\n",
    "            engine=\"openpyxl\"\n",
    "        ).rename(columns={\"TextResponse\": \"response\"})\n",
    "        eval_far_data = eval_far_data[eval_far_data[\"response\"].notna()]\n",
    "        eval_obj_data = eval_obj_data[eval_obj_data[\"response\"].notna()]\n",
    "        return {\n",
    "            \"far\": self.collapse_eval_data(eval_far_data),\n",
    "            \"obj\": self.collapse_eval_data(eval_obj_data),\n",
    "        }\n",
    "#\n",
    "def _expand_response(input_response: str) -> List[str]:\n",
    "    sentences = sent_tokenize(input_response)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEFAULT_TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "            \"distilbert-base-uncased-finetuned-sst-2-english\", use_fast=True)\n",
    "\n",
    "class LDHDataModule(lit.LightningDataModule):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, batch_size=16, tokenizer=DEFAULT_TOKENIZER, strat='obj', kfolds=5, force_reload=False):\n",
    "        super().__init__()\n",
    "        self.strat = strat\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kfolds = kfolds\n",
    "        self.current_split = 0\n",
    "        data = LDHData(self.tokenizer, data_dir) \n",
    "        \n",
    "        self.train_data = self.load_training_data(data, force_reload)\n",
    "        self.indices = self.assign_groups() # Get an array of groups assigned for each data point\n",
    "        self.splits = self.generate_splits(self.indices) # Use GroupKFold to generate specific splits\n",
    "        self.eval_data = self.load_eval_data(data, force_reload)\n",
    "\n",
    "\n",
    "    def load_training_data(self, data:LDHData, force_reload):\n",
    "        data.load_training_data(force_reload)\n",
    "        train_data: Dataset = data.train_dataset[self.strat] \n",
    "        print(\"Encoding Train Data:\")\n",
    "        encoded_ds = train_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"score\"],\n",
    "        )\n",
    "        return encoded_ds\n",
    "\n",
    "    def load_eval_data(self, data:LDHData, force_reload):\n",
    "        data.load_eval_data(force_reload)\n",
    "        eval_data: Dataset = data.eval_dataset[self.strat]\n",
    "        print(\"Encoding Test Data:\")\n",
    "        encoded_ds = eval_data.map(\n",
    "            lambda ds: self.tokenizer(\n",
    "                ds[\"response\"],\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",                    \n",
    "                max_length=150,\n",
    "            )\n",
    "        )\n",
    "        # Need to include addcode here as well\n",
    "        encoded_ds.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], output_all_columns=True\n",
    "        )\n",
    "        return encoded_ds\n",
    "    \n",
    "    def assign_groups(self):\n",
    "        \"\"\" Assign each datapoint to a group for later kfold validation\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.train_data))\n",
    "        indices = indices % self.kfolds\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def generate_splits(self, indices):            \n",
    "        cv = GroupKFold(self.kfolds)\n",
    "        splits = list(cv.split(self.train_data, groups=indices))\n",
    "        return splits\n",
    "\n",
    "    def get_train_dataloader(self, split: int):\n",
    "        train_idx = self.splits[split][0].tolist() # retrieve the split generated by GroupKFold\n",
    "        data = Subset(self.train_data, train_idx)\n",
    "        return DataLoader(data, batch_size=self.batch_size)\n",
    "    \n",
    "    def get_val_dataloader(self, split: int):\n",
    "        val_idx = self.splits[split][1].tolist() # retrieve the split generated by GroupKFold\n",
    "        data = Subset(self.train_data, val_idx)\n",
    "        return DataLoader(data, batch_size=self.batch_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_train_dataloader(self.current_split)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_val_dataloader(self.current_split)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.eval_data, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LDHData.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; \n",
    "notebook2script(\"LDHData.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
