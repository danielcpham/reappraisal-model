{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightningreapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "import torch\n",
    "from pytorch_lightning.metrics.functional import r2score, explained_variance\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "default_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "class LightningReapp(lit.LightningModule):\n",
    "    def __init__(self, config, pretrained_model_name=default_model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.hidden_layer_size = config[\"hidden_layer_size\"]\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize a pretrained model\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Turn off autograd for bert encoder\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, self.hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_layer_size, 7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # define metrics\n",
    "        self.train_loss = lit.metrics.MeanSquaredError()\n",
    "        self.val_loss = lit.metrics.MeanSquaredError()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        avg = get_avg_masked_encoding(last_hidden_state, attention_mask)\n",
    "        out = self.classifier(avg).squeeze()\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # destructure batch\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        score = batch[\"score\"]\n",
    "        # Compute the loss\n",
    "        output = self(input_ids, attention_mask)\n",
    "        loss = self.train_loss(output.sum(dim=1), score)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss)\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        expected = batch[\"score\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        observed = output.sum(dim=1)\n",
    "        loss = self.val_loss(observed, expected)\n",
    "        return {\n",
    "            \"val_loss\": loss,\n",
    "            'r2score': r2score(observed, expected),\n",
    "            'explained_var': explained_variance(observed, expected)\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        r2score = torch.stack([x[\"r2score\"] for x in outputs]).mean()\n",
    "        explained_var = torch.stack([x[\"explained_var\"] for x in outputs]).mean()\n",
    "\n",
    "        # calculate spearman's r and pearson's r\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "        self.log('r2score', r2score.item())\n",
    "        self.log('explained_var', explained_var.item())\n",
    "\n",
    "\n",
    "    # TESTING LOOP\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        # Eval step\n",
    "        return {\"predict\": (batch_idx, output.sum(dim=1))}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        dfs = []\n",
    "        for output in outputs:\n",
    "            batch_idx, result = output['predict']\n",
    "            dfs.append((batch_idx, result.cpu().tolist()))\n",
    "        with open(f\"./output_reapp.pkl\", 'wb+') as f:\n",
    "            pickle.dump(dfs, f)\n",
    "\n",
    "\n",
    "\n",
    "# export\n",
    "def get_avg_masked_encoding(state: torch.Tensor, attention_mask: torch.Tensor):\n",
    "    \"\"\"[summary]\n",
    "    For B = batch size, L = encoding length, F = feature vector:\n",
    "    Args:\n",
    "        state (torch.Tensor): (B, L, F)\n",
    "        attention_mask (torch.Tensor): (B, L)\n",
    "    Returns:\n",
    "        torch.Tensor: (B, F), where L has been masked by `attention_mask` and then averaged.\n",
    "            Each vector in the batch represents the average feature vector for the masked_encoding\n",
    "    \"\"\"\n",
    "    encodings = state.unbind(dim=0)  # split the batch up\n",
    "    new_tensors = []\n",
    "    # TODO: VECTORIZE\n",
    "    for i in range(len(encodings)):\n",
    "        # for each element in the encoding dimension, get the elements that aren't padding using the attention_mask\n",
    "        encoding_mask = attention_mask[i]\n",
    "        # Find the indices where attention_mask > 0 (where the actual tokens are without padding)\n",
    "        indices = encoding_mask.nonzero(as_tuple=True)[0]\n",
    "        masked_encoding = (\n",
    "            encodings[i].index_select(0, indices).squeeze()\n",
    "        )  # torch.Size([len(indices), F])\n",
    "        avg_feature = torch.mean(masked_encoding, dim=0)\n",
    "        new_tensors.append(avg_feature)\n",
    "\n",
    "    return torch.stack(new_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LightningReapp.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib --fname 'LightningReapp.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
