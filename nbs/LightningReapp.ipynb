{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightningreapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "default_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "class LightningReapp(lit.LightningModule):\n",
    "    def __init__(self, config, pretrained_model_name=default_model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.hidden_layer_size = config[\"hidden_layer_size\"]\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize a pretrained model\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Turn off autograd for bert encoder\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, self.hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_layer_size, 7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # define metrics\n",
    "        self.train_loss = lit.metrics.MeanSquaredError()\n",
    "        self.val_loss = lit.metrics.MeanSquaredError()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        avg = get_avg_masked_encoding(last_hidden_state, attention_mask)\n",
    "        out = self.classifier(avg).squeeze()\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # destructure batch\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        score = batch[\"score\"]\n",
    "        # Compute the loss\n",
    "        output = self(input_ids, attention_mask)\n",
    "        loss = self.train_loss(output.sum(dim=1), score)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss)\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        expected = batch[\"score\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        observed = output.sum(dim=1)\n",
    "        loss = self.val_loss(observed, expected)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"expected\": expected,\n",
    "            \"observed\": observed\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "#         observed = torch.stack([x[\"observed\"] for x in outputs])\n",
    "#         expected = torch.stack([x[\"expected\"] for x in outputs])\n",
    "        # calculate spearman's r and pearson's r \n",
    "        #pytorch_lightning.metrics.functional.r2score(preds, target, adjusted=0, multioutput='uniform_average')\n",
    "\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "\n",
    "    # TESTING LOOP\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        # Eval step\n",
    "\n",
    "        result = pd.DataFrame(\n",
    "            {\n",
    "                \"addcode\": batch[\"addcode\"],\n",
    "                \"daycode\": batch[\"daycode\"],\n",
    "                \"condition\": batch[\"Condition\"],\n",
    "                \"response\": batch[\"response\"],\n",
    "                \"output\": output.sum(dim=1),\n",
    "            })\n",
    "        return {\"predict\": result}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        dfs = []\n",
    "        for output in outputs:\n",
    "            dfs.append(output['predict'])\n",
    "        output_df = pd.concat(dfs)\n",
    "        output_df.to_csv(f\"output.csv\")\n",
    "\n",
    "\n",
    "# export\n",
    "def get_avg_masked_encoding(state: torch.Tensor, attention_mask: torch.Tensor):\n",
    "    \"\"\"[summary]\n",
    "    For B = batch size, L = encoding length, F = feature vector:\n",
    "    Args:\n",
    "        state (torch.Tensor): (B, L, F)\n",
    "        attention_mask (torch.Tensor): (B, L)\n",
    "    Returns:\n",
    "        torch.Tensor: (B, F), where L has been masked by `attention_mask` and then averaged.\n",
    "            Each vector in the batch represents the average feature vector for the masked_encoding\n",
    "    \"\"\"\n",
    "    encodings = state.unbind(dim=0)  # split the batch up\n",
    "    new_tensors = []\n",
    "    # TODO: VECTORIZE\n",
    "    for i in range(len(encodings)):\n",
    "        # for each element in the encoding dimension, get the elements that aren't padding using the attention_mask\n",
    "        encoding_mask = attention_mask[i]\n",
    "        # Find the indices where attention_mask > 0 (where the actual tokens are without padding)\n",
    "        indices = encoding_mask.nonzero(as_tuple=True)[0]\n",
    "        masked_encoding = (\n",
    "            encodings[i].index_select(0, indices).squeeze()\n",
    "        )  # torch.Size([len(indices), F])\n",
    "        avg_feature = torch.mean(masked_encoding, dim=0)\n",
    "        new_tensors.append(avg_feature)\n",
    "\n",
    "    return torch.stack(new_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LightningReapp.ipynb.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LightningReapp.ipynb.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
