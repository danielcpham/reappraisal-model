{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightningreapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Model\n",
    "- Model management is handled by PyTorch Lightning. `LightningReapp` is a wrapper instance of a standard PyTorch neural network module such that the training/validation/testing logic is grouped with the logic of a forward pass. \n",
    "- Optimizer: [AdamW](https://pytorch.org/docs/stable/optim.html) \n",
    "- Loss Function: L2 Loss (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pytorch_lightning as lit\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import AutoModel\n",
    "\n",
    "default_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "class LightningReapp(lit.LightningModule):\n",
    "    def __init__(self, config={}, pretrained_model_name=default_model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = config.get('lr', 1e-4)\n",
    "        self.hidden_layer_size = config.get('hidden_layer_size', 50)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize a pretrained model\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Turn off autograd for bert encoder\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, self.hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_layer_size, 7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # define metrics\n",
    "        self.train_loss = lit.metrics.MeanSquaredError()\n",
    "        self.val_loss = lit.metrics.MeanSquaredError()\n",
    "        self.r2score = lit.metrics.R2Score()\n",
    "        self.explained_var = lit.metrics.ExplainedVariance()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        avg = get_avg_masked_encoding(last_hidden_state, attention_mask)\n",
    "        out = self.classifier(avg).squeeze()\n",
    "        return out\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Return whatever optimizers and learning rate schedulers you want here.\n",
    "        At least one optimizer is required.\n",
    "        \"\"\"\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        lr_scheduler = {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': ReduceLROnPlateau(optimizer, mode='min'),\n",
    "                        'name': 'lr',\n",
    "                        \"monitor\": \"val_loss\",\n",
    "                        }\n",
    "        return lr_scheduler\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # destructure batch\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        score = batch[\"score\"]\n",
    "        # Compute the loss\n",
    "        output = self(input_ids, attention_mask)\n",
    "        loss = self.train_loss(output.sum(dim=1), score)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        expected = batch[\"score\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        observed = output.sum(dim=1)\n",
    "        outputs = {\n",
    "            \"val_loss\": self.val_loss(observed, expected),\n",
    "            'lr': self.lr,\n",
    "            'r2score': self.r2score(observed, expected),\n",
    "            'explained_var': self.explained_var(observed, expected)\n",
    "        }\n",
    "        self.log_dict(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss = self.val_loss.compute()\n",
    "        train_loss = self.train_loss.compute()\n",
    "        loss_distance = torch.abs(val_loss - train_loss)\n",
    "        self.log(\"loss_distance\", loss_distance)\n",
    "        self.val_loss.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_avg_masked_encoding(state: torch.Tensor, attention_mask: torch.Tensor):\n",
    "    \"\"\"[summary]\n",
    "    For B = batch size, L = encoding length, F = feature vector:\n",
    "    Args:\n",
    "        state (torch.Tensor): (B, L, F)\n",
    "        attention_mask (torch.Tensor): (B, L)\n",
    "    Returns:\n",
    "        torch.Tensor: (B, F), where L has been masked by `attention_mask` and then averaged.\n",
    "            Each vector in the batch represents the average feature vector for the masked_encoding\n",
    "    \"\"\"\n",
    "    encodings = state.unbind(dim=0)  # split the batch up\n",
    "    new_tensors = []\n",
    "    # TODO: VECTORIZE\n",
    "    for i in range(len(encodings)):\n",
    "        # for each element in the encoding dimension, get the elements that aren't padding using the attention_mask\n",
    "        encoding_mask = attention_mask[i]\n",
    "        # Find the indices where attention_mask > 0 (where the actual tokens are without padding)\n",
    "        indices = encoding_mask.nonzero(as_tuple=True)[0]\n",
    "        masked_encoding = (\n",
    "            encodings[i].index_select(0, indices).squeeze()\n",
    "        )  # torch.Size([len(indices), F])\n",
    "        avg_feature = torch.mean(masked_encoding, dim=0)\n",
    "        new_tensors.append(avg_feature)\n",
    "\n",
    "    return torch.stack(new_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "metadata": {
    "interpreter": {
     "hash": "6cedeabb5f601728273af76a0df97b854fca999a9667201438ccd991d5fa9a34"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
