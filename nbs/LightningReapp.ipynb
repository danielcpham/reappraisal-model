{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightningreapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pytorch_lightning as lit\n",
    "import torch\n",
    "from pytorch_lightning.metrics.functional import r2score, explained_variance\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "default_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "class LightningReapp(lit.LightningModule):\n",
    "    def __init__(self, config, pretrained_model_name=default_model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set model hyperparams\n",
    "        self.lr = config.get(\"lr\", 1e-3)\n",
    "        self.num_embedding_layers = config.get('num_embedding_layers', 1)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize a pretrained model\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            pretrained_model_name,\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "\n",
    "        self.feature_dim = self.bert.config.dim\n",
    "\n",
    "        # Turn off autograd for bert encoder\n",
    "        for param in self.bert.transformer.layer[:-self.num_embedding_layers].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.distance_scorer = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim * self.num_embedding_layers, self.feature_dim),\n",
    "            nn.Linear(self.feature_dim, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "\n",
    "\n",
    "        # define metrics\n",
    "        self.train_loss = lit.metrics.MeanSquaredError()\n",
    "        self.val_loss = lit.metrics.MeanSquaredError()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask, output_hidden_states=True)\n",
    "        last_hidden_states = output.hidden_states[-self.num_embedding_layers:]\n",
    "        states_combined = torch.cat(last_hidden_states, dim=2)\n",
    "        avg = self.pooler(states_combined.permute(0,2,1)).squeeze()\n",
    "        out = self.distance_scorer(avg).squeeze()\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # destructure batch\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        score = batch[\"score\"]\n",
    "        # Compute the loss\n",
    "        output = self(input_ids, attention_mask)\n",
    "        loss = self.train_loss(output, score)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss)\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        expected = batch[\"score\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        observed = output\n",
    "        loss = self.val_loss(observed, expected)\n",
    "        return {\n",
    "            \"val_loss\": loss,\n",
    "            'r2score': r2score(observed, expected),\n",
    "            'explained_var': explained_variance(observed, expected)\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        r2score = torch.stack([x[\"r2score\"] for x in outputs]).mean()\n",
    "        explained_var = torch.stack([x[\"explained_var\"] for x in outputs]).mean()\n",
    "\n",
    "        # calculate spearman's r and pearson's r\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "        self.log('r2score', r2score.item())\n",
    "        self.log('explained_var', explained_var.item())\n",
    "\n",
    "\n",
    "    # TESTING LOOP\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        output = self(input_ids, attention_mask)\n",
    "        # Eval step\n",
    "        return {\"predict\": (batch_idx, output)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        dfs = []\n",
    "        for output in outputs:\n",
    "            batch_idx, result = output['predict']\n",
    "            dfs.append((batch_idx, result.cpu().tolist()))\n",
    "        with open(f\"./output_reapp.pkl\", 'wb+') as f:\n",
    "            pickle.dump(dfs, f)\n",
    "\n",
    "\n",
    "def get_avg_masked_encoding(state: torch.Tensor, attention_mask: torch.Tensor):\n",
    "    \"\"\"[summary]\n",
    "    For B = batch size, L = encoding length, F = feature vector:\n",
    "    Args:\n",
    "        state (torch.Tensor): (B, L, F)\n",
    "        attention_mask (torch.Tensor): (B, L)\n",
    "    Returns:\n",
    "        torch.Tensor: (B, F), where L has been masked by `attention_mask` and then averaged.\n",
    "            Each vector in the batch represents the average feature vector for the masked_encoding\n",
    "    \"\"\"\n",
    "    encodings = state.unbind(dim=0)  # split the batch up\n",
    "    new_tensors = []\n",
    "    # TODO: VECTORIZE\n",
    "    for i in range(len(encodings)):\n",
    "        # for each element in the encoding dimension, get the elements that aren't padding using the attention_mask\n",
    "        encoding_mask = attention_mask[i]\n",
    "        # Find the indices where attention_mask > 0 (where the actual tokens are without padding)\n",
    "        indices = encoding_mask.nonzero(as_tuple=True)[0]\n",
    "        masked_encoding = (\n",
    "            encodings[i].index_select(0, indices).squeeze()\n",
    "        )  # torch.Size([len(indices), F])\n",
    "        avg_feature = torch.mean(masked_encoding, dim=0)\n",
    "        new_tensors.append(avg_feature)\n",
    "\n",
    "    return torch.stack(new_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.2.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1536])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-28.8698, -28.1979, -29.1671], grad_fn=<MvBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "output = model.forward(model.dummy_inputs['input_ids'])\n",
    "\n",
    "hidden_states = output.hidden_states\n",
    "num_hidden_states = 2\n",
    "last_two = hidden_states[-num_hidden_states:]\n",
    "## Concatenate the last two states\n",
    "last_two_concat = torch.cat(last_two, dim=2)\n",
    "last_two_concat.shape\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "avg = F.avg_pool1d(\n",
    "    last_two_concat.permute(0,2,1), \n",
    "    kernel_size=5, \n",
    "    count_include_pad=False).squeeze()\n",
    "\n",
    "print(avg.shape)\n",
    "F.linear(avg, weight=torch.rand(avg.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LightningReapp.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script('LightningReapp.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7': pyenv)",
   "metadata": {
    "interpreter": {
     "hash": "6cedeabb5f601728273af76a0df97b854fca999a9667201438ccd991d5fa9a34"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
