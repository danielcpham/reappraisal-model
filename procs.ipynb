{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- When running on Google Colab, mount Google Drive to access scripts.\n",
    "- `cd` into the project root and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd {root_dir}\n",
    "\n",
    "# %pip install pytorch-lightning \"ray[tune]\" wandb transformers datasets nltk nbdev jupyterlab_github boto3\n",
    "# ! nbdev_install_git_hooks\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# ROOT_DIR = '/root/reappraisal-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define constants\n",
    "STRAT = 'obj'\n",
    "BATCH_SIZE = 64 \n",
    "NUM_FOLDS=1\n",
    "# ROOT_DIR= "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDH Data\n",
    "\n",
    "Contains the following:\n",
    "\n",
    "- LDHI\n",
    "- LDHII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "ldhdata = LDHDataModule(data_dir='/Users/danielpham/Google Drive/ldh', strat=STRAT)\n",
    "ldhdata.load_train_data()\n",
    "ldhdata.load_eval_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run K-Fold Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from reappraisalmodel.trainers import kfold_train\n",
    "\n",
    "\n",
    "#TODO: ADD CSVLOGGER!!!!\n",
    "results = kfold_train(5, ldhdata, strat=STRAT, \n",
    "                       max_epochs=15, \n",
    "#                        limit_train_batches=2,\n",
    "#                        limit_val_batches=1\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['r2score'] = df['r2score'].apply(lambda x: x.item())\n",
    "df['explained_var'] = df['explained_var'].apply(lambda x: x.item())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "%autoreload\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler # https://arxiv.org/abs/1810.05934\n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_tune_config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1), # loguniform samples by magnitude\n",
    "    \"hidden_layer_size\": tune.randint(0,50),\n",
    "}\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1)\n",
    "\n",
    "reporter = JupyterNotebookReporter(max_progress_rows=10)\n",
    "\n",
    "callback_tuner = TuneReportCallback(\n",
    "    {\n",
    "        \"loss\": \"val_loss\",\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "\n",
    "### TUNING HYPERPARAMETERS\n",
    "def train_tune(config, ldhdata, num_gpus=None, num_epochs=10):\n",
    "    model = LightningReapp(config)\n",
    "    print(\"Running tune\")\n",
    "    trainer = lit.Trainer(\n",
    "        limit_train_batches=1,\n",
    "        limit_val_batches=1,\n",
    "        gpus=num_gpus,\n",
    "        callbacks=[callback_tuner],\n",
    "    )\n",
    "    trainer.fit(model, ldhdata)\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(train_tune,\n",
    "        ldhdata=ldhdata,\n",
    "        num_epochs=1),\n",
    "    config=default_tune_config, \n",
    "    num_samples=2)\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import pytorch_lightning as lit\n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "model = LightningReapp({\n",
    "    'lr': 1e-3,\n",
    "    'hidden_layer_size': 50\n",
    "})\n",
    "\n",
    "trainer = lit.Trainer(fast_dev_run=1)\n",
    "trainer.fit(model, ldhdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.list_objects(Bucket='ldhdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"obj/{'metrics': {'epoch': tensor(10.), 'val_loss': tensor(1.2882), 'r2score': tensor(0.5832), 'explained_var': tensor(0.6109), 'train_loss': tensor(1.1194, device='cuda:0')}, 'checkpoint': '/tmp/tmpvsrtlvfl/reappmodel_obj_20210227_203013/2_epoch=07-val_loss=1.12.ckpt', 'num_epochs': 10}-20210227_203013-2_epoch=07-val_loss=1.12.ckpt\"\n",
    "s3.copy({\n",
    "    'Bucket': 'ldhdata',\n",
    "    'Key': key\n",
    "},\n",
    "Bucket='ldhdata',\n",
    "Key='obj/20210227_203013-2_epoch=07-val_loss=1.12.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "metrics = [\n",
    "    {'epoch': torch.tensor(7.), 'val_loss': 1.1842, 'r2score': torch.tensor(0.6130), 'explained_var': torch.tensor(0.6388), 'train_loss': torch.tensor(1.1642, device='cuda:0')},\n",
    "    {'epoch': torch.tensor(8.), 'val_loss': 1.1819, 'r2score': torch.tensor(0.6087), 'explained_var': torch.tensor(0.6377), 'train_loss': torch.tensor(1.0931, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(9.), 'val_loss': 1.2094, 'r2score': torch.tensor(0.5926), 'explained_var': torch.tensor(0.6366), 'train_loss': torch.tensor(1.1363, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2712, 'r2score': torch.tensor(0.5906), 'explained_var': torch.tensor(0.6339), 'train_loss': torch.tensor(1.0842, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2882, 'r2score': torch.tensor(0.5832), 'explained_var': torch.tensor(0.6109), 'train_loss': torch.tensor(1.1194, device='cuda:0')}, \n",
    "]\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "for key in ['r2score', 'epoch', 'explained_var', 'train_loss']:\n",
    "    df[key] = df[key].apply(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_report = upload_file('this.csv', 'ldhdata', f'obj/20210227_203013-report.csv')\n",
    "print(f\"Successful Uploading Report to s3: {upload_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in resp['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Returns a BatchEncoding of the text.\n",
    "tokenized = tokenizer(text = [\"This is the first test sentence!\", \"This is the second, better test sentence.\"], \n",
    "    padding='max_length', max_length=150)\n",
    "\n",
    "for idx, sent in enumerate(tokenized.input_ids):\n",
    "    print(f\"Sentence            {idx}: {tokenizer.convert_ids_to_tokens(sent)}\")\n",
    "    print(f\"Tokenized Attention {idx}: {tokenized[idx].attention_mask}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_config = default_config = {\n",
    "    'lr': 1e-3,\n",
    "    'hidden_lay': 50\n",
    "}\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer = lit.Trainer(\n",
    "    gpus = 1 if torch.cuda.is_available() else None,\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    max_epochs=10,\n",
    "    fast_dev_run=2,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer.fit(model, ldhdata.train_dataloader(), ldhdata.val_dataloader())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningReapp.load_from_checkpoint(\n",
    "    '/Users/danielpham/Google Drive/ldh/lightning_logs_obj_0223/version_2/checkpoints/epoch=1-step=337.ckpt', map_location='cpu')\n",
    "\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of reappraisalmodel.ldhdata failed: Traceback (most recent call last):\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: prepare_data() requires a code object with 1 free vars, not 0\n",
      "]\n",
      "2021-03-02 22:26:22,239\tWARNING worker.py:1107 -- The log monitor on node Daniels-MBP failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/redis/connection.py\", line 706, in send_packed_command\n",
      "    sendall(self._sock, item)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/redis/_compat.py\", line 9, in sendall\n",
      "    return sock.sendall(*args, **kwargs)\n",
      "OSError: [Errno 41] Protocol wrong type for socket\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/ray/log_monitor.py\", line 359, in <module>\n",
      "    log_monitor.run()\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/ray/log_monitor.py\", line 281, in run\n",
      "    anything_published = self.check_log_files_and_publish_updates()\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/ray/log_monitor.py\", line 259, in check_log_files_and_publish_updates\n",
      "    self.redis_client.publish(\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/redis/client.py\", line 3098, in publish\n",
      "    return self.execute_command('PUBLISH', channel, message)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/redis/client.py\", line 900, in execute_command\n",
      "    conn.send_command(*args)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/redis/connection.py\", line 725, in send_command\n",
      "    self.send_packed_command(self.pack_command(*args),\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/redis/connection.py\", line 717, in send_packed_command\n",
      "    raise ConnectionError(\"Error %s while writing to socket. %s.\" %\n",
      "redis.exceptions.ConnectionError: Error 41 while writing to socket. Protocol wrong type for socket.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DistilBertModel' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-375817c49557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightningReapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m trainer = Trainer(\n\u001b[1;32m     14\u001b[0m     \u001b[0mgradient_clip_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/ldh/reappraisalmodel/lightningreapp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pretrained_model_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Turn off autograd for bert encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embedding_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DistilBertModel' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "%autoreload \n",
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer \n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "config = {\n",
    "    'lr': 1e-3,\n",
    "    'num_embedding_layers': 2\n",
    "}\n",
    "\n",
    "model = LightningReapp(config)\n",
    "trainer = Trainer(\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "test_dataloader = ldhdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for result in results:\n",
    "    print(len(result['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'ldhdata'\n",
    "file = 'Master_Final_TrainingData.csv'\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "response = s3client.get_object(Bucket=bucket, Key=file)\n",
    "\n",
    "import codecs \n",
    "import csv\n",
    "\n",
    "train = csv.DictReader(codecs.getreader(\"utf-8\")(response[\"Body\"])) # returns an ordered dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7': pyenv)",
   "metadata": {
    "interpreter": {
     "hash": "6cedeabb5f601728273af76a0df97b854fca999a9667201438ccd991d5fa9a34"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
