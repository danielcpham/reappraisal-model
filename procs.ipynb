{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- When running on Google Colab, mount Google Drive to access scripts.\n",
    "- `cd` into the project root and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "\n",
    "# Define constants\n",
    "STRAT = 'obj'\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-04 10:26:01,805\tERROR worker.py:1109 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2021-03-04 10:26:01,809\tERROR worker.py:919 -- print_logs: Connection closed by server.\n",
      "2021-03-04 10:26:01,809\tERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDH Data\n",
    "\n",
    "Contains the following:\n",
    "\n",
    "- LDHI\n",
    "- LDHII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/reapp/output/training/obj/cache-8217e0ac99d256e6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded from disk.\n",
      "Encoding Training Data:\n",
      "Evaluation data loaded from disk.\n",
      "Encoding Test Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545bd17495cd456689e8e8c0f2345eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32109.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "ldhdata = LDHDataModule(data_dir=ROOT_DIR, strat=STRAT)\n",
    "ldhdata.load_train_data()\n",
    "ldhdata.load_eval_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Run K-Fold Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmpq71exrki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on split 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Experiment logs directory reapp_logs/01foldCV_obj_20210304_091945/split_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d3fce5da8742f795a355f4d9b6b4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successful 0_epoch=00-val_loss=6.23.ckpt to s3: True\n",
      "{'val_loss': 6.233573913574219, 'train_loss': 23.186851501464844, 'num_epochs': 0, 'r2score': tensor(-0.5544), 'explained_var': tensor(-0.3108)}\n",
      "       val_loss  train_loss  num_epochs   r2score  explained_var\n",
      "count  1.000000    1.000000         1.0  1.000000        1.00000\n",
      "mean   6.233574   23.186852         0.0 -0.554429       -0.31076\n",
      "std         NaN         NaN         NaN       NaN            NaN\n",
      "min    6.233574   23.186852         0.0 -0.554429       -0.31076\n",
      "25%    6.233574   23.186852         0.0 -0.554429       -0.31076\n",
      "50%    6.233574   23.186852         0.0 -0.554429       -0.31076\n",
      "75%    6.233574   23.186852         0.0 -0.554429       -0.31076\n",
      "max    6.233574   23.186852         0.0 -0.554429       -0.31076\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from reappraisalmodel.trainers import kfold_train\n",
    "\n",
    "results = kfold_train(1, ldhdata, strat=STRAT, \n",
    "        max_epochs=1,\n",
    "        limit_train_batches=1,\n",
    "        limit_val_batches=1,\n",
    "        s3_bucket=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['r2score'] = df['r2score'].apply(lambda x: x.item())\n",
    "df['explained_var'] = df['explained_var'].apply(lambda x: x.item())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-04 09:48:23,292\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-03-04 09:48:25,036\tWARNING function_runner.py:540 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 2.1/59.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0.25/1 GPUs, 0.0/35.25 GiB heap, 0.0/12.16 GiB objects (0/1.0 accelerator_type:K80)\n",
      "Result logdir: /home/ubuntu/reapp/reapp_logs/tune/_inner_2021-03-04_09-48-25\n",
      "Number of trials: 1/2 (1 RUNNING)\n",
      "+--------------------+----------+-------+-------------+\n",
      "| Trial name         | status   | loc   |          lr |\n",
      "|--------------------+----------+-------+-------------|\n",
      "| _inner_c38a9_00000 | RUNNING  |       | 0.000782566 |\n",
      "+--------------------+----------+-------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m 2021-03-04 09:48:29,682\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 576, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 651, in _inner\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 645, in inner\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     fn(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"<ipython-input-5-ca4dd13e2d68>\", line 33, in train_tune\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     return fn(self, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 318, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self.accelerator_connector = AcceleratorConnector(\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 117, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 81, in parse_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     gpus = _sanitize_gpu_ids(gpus)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 142, in _sanitize_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     raise MisconfigurationException(\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0]\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m  But your machine only has: []\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 576, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 651, in _inner\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 645, in inner\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     fn(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"<ipython-input-5-ca4dd13e2d68>\", line 33, in train_tune\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     return fn(self, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 318, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self.accelerator_connector = AcceleratorConnector(\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 117, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 81, in parse_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     gpus = _sanitize_gpu_ids(gpus)\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 142, in _sanitize_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m     raise MisconfigurationException(\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0]\n",
      "\u001b[2m\u001b[36m(pid=17126)\u001b[0m  But your machine only has: []\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m 2021-03-04 09:48:29,708\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 576, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 651, in _inner\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 645, in inner\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     fn(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"<ipython-input-5-ca4dd13e2d68>\", line 33, in train_tune\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     return fn(self, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 318, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self.accelerator_connector = AcceleratorConnector(\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 117, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 81, in parse_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     gpus = _sanitize_gpu_ids(gpus)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 142, in _sanitize_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     raise MisconfigurationException(\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0]\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m  But your machine only has: []\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 576, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 651, in _inner\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 645, in inner\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     fn(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"<ipython-input-5-ca4dd13e2d68>\", line 33, in train_tune\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     return fn(self, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 318, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self.accelerator_connector = AcceleratorConnector(\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 117, in __init__\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 81, in parse_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     gpus = _sanitize_gpu_ids(gpus)\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m   File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 142, in _sanitize_gpu_ids\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m     raise MisconfigurationException(\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0]\n",
      "\u001b[2m\u001b[36m(pid=17129)\u001b[0m  But your machine only has: []\n",
      "2021-03-04 09:48:29,762\tERROR trial_runner.py:616 -- Trial _inner_c38a9_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 586, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 609, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=17126, ip=172.31.10.247)\n",
      "  File \"python/ray/_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/trainable.py\", line 167, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/trainable.py\", line 226, in train\n",
      "    result = self.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError(\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=17126, ip=172.31.10.247)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 576, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 651, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 645, in inner\n",
      "    fn(config, **fn_kwargs)\n",
      "  File \"<ipython-input-5-ca4dd13e2d68>\", line 33, in train_tune\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\n",
      "    return fn(self, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 318, in __init__\n",
      "    self.accelerator_connector = AcceleratorConnector(\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 117, in __init__\n",
      "    self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 81, in parse_gpu_ids\n",
      "    gpus = _sanitize_gpu_ids(gpus)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 142, in _sanitize_gpu_ids\n",
      "    raise MisconfigurationException(\n",
      "pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0]\n",
      " But your machine only has: []\n",
      "2021-03-04 09:48:29,768\tERROR trial_runner.py:616 -- Trial _inner_c38a9_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 586, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 609, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=17129, ip=172.31.10.247)\n",
      "  File \"python/ray/_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/trainable.py\", line 167, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/trainable.py\", line 226, in train\n",
      "    result = self.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError(\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=17129, ip=172.31.10.247)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 576, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 651, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 645, in inner\n",
      "    fn(config, **fn_kwargs)\n",
      "  File \"<ipython-input-5-ca4dd13e2d68>\", line 33, in train_tune\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\n",
      "    return fn(self, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 318, in __init__\n",
      "    self.accelerator_connector = AcceleratorConnector(\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 117, in __init__\n",
      "    self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 81, in parse_gpu_ids\n",
      "    gpus = _sanitize_gpu_ids(gpus)\n",
      "  File \"/home/ubuntu/anaconda3/envs/reapp/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\", line 142, in _sanitize_gpu_ids\n",
      "    raise MisconfigurationException(\n",
      "pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0]\n",
      " But your machine only has: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for _inner_c38a9_00001:\n",
      "  {}\n",
      "  \n",
      "Result for _inner_c38a9_00000:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 3.0/59.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0.0/1 GPUs, 0.0/35.25 GiB heap, 0.0/12.16 GiB objects (0/1.0 accelerator_type:K80)\n",
      "Result logdir: /home/ubuntu/reapp/reapp_logs/tune/_inner_2021-03-04_09-48-25\n",
      "Number of trials: 2/2 (2 ERROR)\n",
      "+--------------------+----------+-------+-------------+\n",
      "| Trial name         | status   | loc   |          lr |\n",
      "|--------------------+----------+-------+-------------|\n",
      "| _inner_c38a9_00000 | ERROR    |       | 0.000782566 |\n",
      "| _inner_c38a9_00001 | ERROR    |       | 0.0125501   |\n",
      "+--------------------+----------+-------+-------------+\n",
      "Number of errored trials: 2\n",
      "+--------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name         |   # failures | error file                                                                                                                                            |\n",
      "|--------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _inner_c38a9_00000 |            1 | /home/ubuntu/reapp/reapp_logs/tune/_inner_2021-03-04_09-48-25/_inner_c38a9_00000_0_lr=0.00078257,num_embedding_layers=2_2021-03-04_09-48-25/error.txt |\n",
      "| _inner_c38a9_00001 |            1 | /home/ubuntu/reapp/reapp_logs/tune/_inner_2021-03-04_09-48-25/_inner_c38a9_00001_1_lr=0.01255,num_embedding_layers=2_2021-03-04_09-48-25/error.txt    |\n",
      "+--------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [_inner_c38a9_00000, _inner_c38a9_00001])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ca4dd13e2d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mldhdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m analysis = tune.run(\n\u001b[0m\u001b[1;32m     42\u001b[0m     tune.with_parameters(train_tune,\n\u001b[1;32m     43\u001b[0m         \u001b[0mldhdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldhdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/reapp/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [_inner_c38a9_00000, _inner_c38a9_00001])"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import pytorch_lightning as lit\n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "model = LightningReapp({\n",
    "    'lr': 1e-3,\n",
    "    'hidden_layer_size': 50\n",
    "})\n",
    "\n",
    "trainer = lit.Trainer(fast_dev_run=1)\n",
    "trainer.fit(model, ldhdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.list_objects(Bucket='ldhdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"obj/{'metrics': {'epoch': tensor(10.), 'val_loss': tensor(1.2882), 'r2score': tensor(0.5832), 'explained_var': tensor(0.6109), 'train_loss': tensor(1.1194, device='cuda:0')}, 'checkpoint': '/tmp/tmpvsrtlvfl/reappmodel_obj_20210227_203013/2_epoch=07-val_loss=1.12.ckpt', 'num_epochs': 10}-20210227_203013-2_epoch=07-val_loss=1.12.ckpt\"\n",
    "s3.copy({\n",
    "    'Bucket': 'ldhdata',\n",
    "    'Key': key\n",
    "},\n",
    "Bucket='ldhdata',\n",
    "Key='obj/20210227_203013-2_epoch=07-val_loss=1.12.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "metrics = [\n",
    "    {'epoch': torch.tensor(7.), 'val_loss': 1.1842, 'r2score': torch.tensor(0.6130), 'explained_var': torch.tensor(0.6388), 'train_loss': torch.tensor(1.1642, device='cuda:0')},\n",
    "    {'epoch': torch.tensor(8.), 'val_loss': 1.1819, 'r2score': torch.tensor(0.6087), 'explained_var': torch.tensor(0.6377), 'train_loss': torch.tensor(1.0931, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(9.), 'val_loss': 1.2094, 'r2score': torch.tensor(0.5926), 'explained_var': torch.tensor(0.6366), 'train_loss': torch.tensor(1.1363, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2712, 'r2score': torch.tensor(0.5906), 'explained_var': torch.tensor(0.6339), 'train_loss': torch.tensor(1.0842, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2882, 'r2score': torch.tensor(0.5832), 'explained_var': torch.tensor(0.6109), 'train_loss': torch.tensor(1.1194, device='cuda:0')}, \n",
    "]\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "for key in ['r2score', 'epoch', 'explained_var', 'train_loss']:\n",
    "    df[key] = df[key].apply(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_report = upload_file('this.csv', 'ldhdata', f'obj/20210227_203013-report.csv')\n",
    "print(f\"Successful Uploading Report to s3: {upload_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in resp['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Returns a BatchEncoding of the text.\n",
    "tokenized = tokenizer(text = [\"This is the first test sentence!\", \"This is the second, better test sentence.\"], \n",
    "    padding='max_length', max_length=150)\n",
    "\n",
    "for idx, sent in enumerate(tokenized.input_ids):\n",
    "    print(f\"Sentence            {idx}: {tokenizer.convert_ids_to_tokens(sent)}\")\n",
    "    print(f\"Tokenized Attention {idx}: {tokenized[idx].attention_mask}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_config = default_config = {\n",
    "    'lr': 1e-3,\n",
    "    'hidden_lay': 50\n",
    "}\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer = lit.Trainer(\n",
    "    gpus = 1 if torch.cuda.is_available() else None,\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    max_epochs=10,\n",
    "    fast_dev_run=2,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer.fit(model, ldhdata.train_dataloader(), ldhdata.val_dataloader())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningReapp.load_from_checkpoint(\n",
    "    '/Users/danielpham/Google Drive/ldh/lightning_logs_obj_0223/version_2/checkpoints/epoch=1-step=337.ckpt', map_location='cpu')\n",
    "\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload \n",
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer \n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "config = {\n",
    "    'lr': 1e-3,\n",
    "    'num_embedding_layers': 2\n",
    "}\n",
    "\n",
    "model = LightningReapp(config)\n",
    "trainer = Trainer(\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "test_dataloader = ldhdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for result in results:\n",
    "    print(len(result['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'ldhdata'\n",
    "file = 'Master_Final_TrainingData.csv'\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "response = s3client.get_object(Bucket=bucket, Key=file)\n",
    "\n",
    "import codecs \n",
    "import csv\n",
    "\n",
    "train = csv.DictReader(codecs.getreader(\"utf-8\")(response[\"Body\"])) # returns an ordered dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
