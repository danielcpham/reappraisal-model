{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- When running on Google Colab, mount Google Drive to access scripts.\n",
    "- `cd` into the project root and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd {root_dir}\n",
    "\n",
    "# %pip install pytorch-lightning \"ray[tune]\" wandb transformers datasets nltk nbdev jupyterlab_github\n",
    "# ! nbdev_install_git_hooks\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# ROOT_DIR = '/root/reappraisal-model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "STRAT = 'obj'\n",
    "BATCH_SIZE = 64 \n",
    "NUM_FOLDS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDH Data\n",
    "\n",
    "Contains the following:\n",
    "\n",
    "- LDHI\n",
    "- LDHII\n",
    "- LDHIII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of reappraisalmodel.ldhdata failed: Traceback (most recent call last):\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/danielpham/.pyenv/versions/3.8.7/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: prepare_data() requires a code object with 1 free vars, not 0\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "ldhdata = LDHDataModule(data_dir=ROOT_DIR, batch_size=BATCH_SIZE, strat=STRAT, kfolds=5)\n",
    "ldhdata.load_train_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run K-Fold Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from reappraisalmodel.trainers import kfold_train\n",
    "\n",
    "\n",
    "#TODO: ADD CSVLOGGER!!!!\n",
    "results = kfold_train(5, ldhdata, strat=STRAT, \n",
    "                       max_epochs=15, \n",
    "#                        limit_train_batches=2,\n",
    "#                        limit_val_batches=1\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['r2score'] = df['r2score'].apply(lambda x: x.item())\n",
    "df['explained_var'] = df['explained_var'].apply(lambda x: x.item())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(0.5976)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldhdata = LDHDataModule(data_dir=ROOT_DIR, strat=STRAT, kfolds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a9b352f17624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightningReapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mldhdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36moverwrite_by_env_vars\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moverwrite_by_env_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from functools import partial\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray import tune\n",
    "\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_tune_config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1), # loguniform samples by magnitude\n",
    "    \"hidden_layer_size\": tune.quniform(10, 50, 1)\n",
    "}\n",
    "\n",
    "# scheduler = ASHAScheduler(\n",
    "#     max_t=num_epochs,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2)\n",
    "\n",
    "callback_tuner = TuneReportCallback(\n",
    "    {\n",
    "        \"loss\": \"val_loss\",\n",
    "        # \"mean_accuracy\": \"val_accuracy\"\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### TUNING HYPERPARAMETERS\n",
    "def train_tune(config, num_gpus=None, **tuner_kwargs):\n",
    "    model = LightningReapp(config)\n",
    "\n",
    "    trainer = lit.Trainer(\n",
    "        gpus= num_gpus,\n",
    "        callbacks=[callback_tuner],\n",
    "    )\n",
    "    trainer.fit(model, ldhdata)\n",
    "\n",
    "\n",
    "analysis = tune.run(tune.with_parameters(train_tune,\n",
    "                                        ),\n",
    "                    config=default_tune_config, \n",
    "                    num_samples=2, \n",
    "                    resources_per_trial={\n",
    "                        'cpu'=1,\n",
    "                        gpus\n",
    "                    }\n",
    "             scheduler=scheduler)\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.list_objects(Bucket='ldhdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"obj/{'metrics': {'epoch': tensor(10.), 'val_loss': tensor(1.2882), 'r2score': tensor(0.5832), 'explained_var': tensor(0.6109), 'train_loss': tensor(1.1194, device='cuda:0')}, 'checkpoint': '/tmp/tmpvsrtlvfl/reappmodel_obj_20210227_203013/2_epoch=07-val_loss=1.12.ckpt', 'num_epochs': 10}-20210227_203013-2_epoch=07-val_loss=1.12.ckpt\"\n",
    "s3.copy({\n",
    "    'Bucket': 'ldhdata',\n",
    "    'Key': key\n",
    "},\n",
    "Bucket='ldhdata',\n",
    "Key='obj/20210227_203013-2_epoch=07-val_loss=1.12.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "metrics = [\n",
    "    {'epoch': torch.tensor(7.), 'val_loss': 1.1842, 'r2score': torch.tensor(0.6130), 'explained_var': torch.tensor(0.6388), 'train_loss': torch.tensor(1.1642, device='cuda:0')},\n",
    "    {'epoch': torch.tensor(8.), 'val_loss': 1.1819, 'r2score': torch.tensor(0.6087), 'explained_var': torch.tensor(0.6377), 'train_loss': torch.tensor(1.0931, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(9.), 'val_loss': 1.2094, 'r2score': torch.tensor(0.5926), 'explained_var': torch.tensor(0.6366), 'train_loss': torch.tensor(1.1363, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2712, 'r2score': torch.tensor(0.5906), 'explained_var': torch.tensor(0.6339), 'train_loss': torch.tensor(1.0842, device='cuda:0')}, \n",
    "    {'epoch': torch.tensor(10.), 'val_loss': 1.2882, 'r2score': torch.tensor(0.5832), 'explained_var': torch.tensor(0.6109), 'train_loss': torch.tensor(1.1194, device='cuda:0')}, \n",
    "]\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "for key in ['r2score', 'epoch', 'explained_var', 'train_loss']:\n",
    "    df[key] = df[key].apply(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "upload_report = upload_file('this.csv', 'ldhdata', f'obj/20210227_203013-report.csv')\n",
    "print(f\"Successful Uploading Report to s3: {upload_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in resp['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(pretrained_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Returns a BatchEncoding of the text.\n",
    "tokenized = tokenizer(text = [\"This is the first test sentence!\", \"This is the second, better test sentence.\"], \n",
    "    padding='max_length', max_length=150)\n",
    "\n",
    "for idx, sent in enumerate(tokenized.input_ids):\n",
    "    print(f\"Sentence            {idx}: {tokenizer.convert_ids_to_tokens(sent)}\")\n",
    "    print(f\"Tokenized Attention {idx}: {tokenized[idx].attention_mask}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "\n",
    "default_config = default_config = {\n",
    "    'lr': 1e-3,\n",
    "    'hidden_layer_size': 50\n",
    "}\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer = lit.Trainer(\n",
    "    gpus = 1 if torch.cuda.is_available() else None,\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    max_epochs=10,\n",
    "    fast_dev_run=2,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model = LightningReapp(default_config)\n",
    "\n",
    "trainer.fit(model, ldhdata.train_dataloader(), ldhdata.val_dataloader())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningReapp.load_from_checkpoint(\n",
    "    '/Users/danielpham/Google Drive/ldh/lightning_logs_obj_0223/version_2/checkpoints/epoch=1-step=337.ckpt', map_location='cpu')\n",
    "\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer \n",
    "\n",
    "model.eval()\n",
    "trainer = Trainer(\n",
    "    gradient_clip_val=1.0,\n",
    "    progress_bar_refresh_rate=30,\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "test_dataloader = ldhdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for result in results:\n",
    "    print(len(result['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./output_reapp.pkl', 'rb+') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'ldhdata'\n",
    "file = 'Master_Final_TrainingData.csv'\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "response = s3client.get_object(Bucket=bucket, Key=file)\n",
    "\n",
    "import codecs \n",
    "import csv\n",
    "\n",
    "train = csv.DictReader(codecs.getreader(\"utf-8\")(response[\"Body\"])) # returns an ordered dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning.metrics.functional import r2score, explained_variance\n",
    "\n",
    "expected = torch.rand(16)\n",
    "observed = torch.rand(16)\n",
    "\n",
    "r2score(expected, observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "metadata": {
    "interpreter": {
     "hash": "6cedeabb5f601728273af76a0df97b854fca999a9667201438ccd991d5fa9a34"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
