{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reappraisal Training on PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- Required Python Version: 3.7+\n",
    "- `cd` into the project root and install dependencies:\n",
    "  - `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab Setup\n",
    "- When the repository is stored on Google Drive, it can be accessed using Google Colaboratory. The cell below mounts the drive, installs the necessary packages, and changes the root directory to the project directory.\n",
    "    - Python Version: `3.7.10`\n",
    "    - Be sure to change the name of the project directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When Running on Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %pip install pytorch-lightning \"ray[tune]\" wandb transformers datasets nltk nbdev\n",
    "# ! nbdev_install_git_hooks\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "PROJECT_NAME = \"ldh\"\n",
    "ROOT_DIR = f\"/content/drive/MyDrive/{PROJECT_NAME}\"\n",
    "%cd {ROOT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Encoding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define project root directory.\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "# Select the proper strategy. Valid strategy names: \"obj\", \"far\"\n",
    "STRAT = 'obj'\n",
    "# Define batch size.\n",
    "BATCH_SIZE = 64\n",
    "DEV_FLAG = True # Flag for fast runs when debugging.\n",
    "\n",
    "# Load the DataModule and its corresponding \n",
    "from reappraisalmodel.ldhdata import LDHDataModule\n",
    "ldhdata = LDHDataModule(data_dir=ROOT_DIR, strat=STRAT)\n",
    "ldhdata.load_train_data()\n",
    "ldhdata.load_eval_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "Loading a `LightningReapp` model without any arguments will load a model with uninitialized parameters (that is, a blank, untrained reappraisal odel). We can load any valid LightningReapp model using a checkpoint file like so:\n",
    "```python\n",
    "ckpt_path = # Any path on local storage or remote storage (i.e. s3)\n",
    "model = LightningReapp.load_from_checkpoint(ckpt_path)\n",
    "```\n",
    "If loading a model from s3, the `s3fs` package should be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reappraisalmodel.lightningreapp import LightningReapp\n",
    "model = LightningReapp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from reappraisalmodel.trainers import kfold_train\n",
    "# When running k-fold cross-validation, define the number of folds. \n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Learns a model NUM_FOLDS times and records the distribution of metrics across the CV.\n",
    "results = kfold_train(\n",
    "    NUM_FOLDS, \n",
    "    ldhdata, \n",
    "    strat=STRAT\n",
    ")\n",
    "df = pd.DataFrame(results)\n",
    "df['r2score'] = df['r2score'].apply(lambda x: x.item())\n",
    "df['explained_var'] = df['explained_var'].apply(lambda x: x.item())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run K-Fold Training\n",
    "Runs K-Fold Cross-Validation on the training algorithm. Reports the distribution of training results for each fold. \n",
    "- See [Trainers.ipynb](./nbs/Trainers.ipynb) for more information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process\n",
    "\n",
    "### A Note on GPUs:\n",
    "- It is infeasible to train most machine learning models on a CPU, with a single training epoch (a pass though the training data) taking on the order of hours. GPUs enable fast computation because they're optimized for matrix operations. Listed below are popular services that provide GPU usage with built-in Jupyter Notebook integration:\n",
    "  - Amazon Web Services\n",
    "  - Kaggle\n",
    "  - Google Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Training Session\n",
    "Defines the process of running a training session for `LightningReapp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as lit\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saves the 3 checkpoints with the lowest validation loss throughout training\n",
    "modelcheckpoint = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    verbose=True\n",
    ")\n",
    "# Model tracks the loss_distance; shows when training and validation loss begin to diverge \n",
    "modelcheckpoint_loss_dist = ModelCheckpoint(\n",
    "    monitor='loss_distance',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Split train and validation data.\n",
    "split_data = ldhdata.train_data.train_test_split(test_size=0.2)\n",
    "train_data = split_data['train'].with_format(type='torch', columns=['score', 'input_ids', 'attention_mask'])\n",
    "val_data = split_data['test'].with_format(type='torch', columns=['score', 'input_ids', 'attention_mask'])\n",
    "eval_data = ldhdata.eval_data.with_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "val_dl = DataLoader(val_data, batch_size=BATCH_SIZE )\n",
    "eval_dl = DataLoader(eval_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Mark the start time of the training session. \n",
    "today = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "session_version = \"_\".join([STRAT,today])\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\", name=\"reapp_model\", version=session_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "PyTorch Lightning's [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) abstracts aspects of the trainer loop configuration not related to the model. This includes registering callback functions, stop conditions, GPU/CPU configuation, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = lit.Trainer(\n",
    "    logger = tb_logger,\n",
    "    precision=16 if torch.cuda.is_available() else 32, # We use 16-bit precision to reduce computational complexity\n",
    "    val_check_interval=0.25, # Check validation loss 4 times an epoch\n",
    "    callbacks=[modelcheckpoint, modelcheckpoint_loss_dist], # Register callbacks with trainer.\n",
    "    gpus=1 if torch.cuda.is_available() else None,\n",
    ")\n",
    "# Fit the model to the training data. \n",
    "results = trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Study 1 Data\n",
    "\n",
    "The following script predicts the reappraisal score for a trained model on the designated training/validation dataset.\n",
    "\n",
    "Study 1 Data is structured as follows:\n",
    "- `response`: The participant's written response to the study's stimuli.\n",
    "- `score`: Each response was rated by numerous raters on a scale of 1-7, with a higher score corresponding to a higher usage of a specific reappraisal strategy (objective distancing vs. spatiotemporal distancing). The ratings are then averaged. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Study 2 Data\n",
    "\n",
    "The following script predicts the reappraisal score for a trained model on the designated testing dataset.\n",
    "\n",
    "Study 2 Data is structured as follows:\n",
    "- `response`: The participant's written response to the study's stimuli.\n",
    "- `Condition`: Describes the type of stimulus the participant reacts to.\n",
    "- `addcode`: Subject Identification\n",
    "- `daycode`: The day in the study the response was recorded for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "for idx, batch in enumerate(tqdm(eval_dl)):\n",
    "    if DEV_FLAG and idx >= 2:\n",
    "        break\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    out = model(input_ids, attention_mask)\n",
    "    outs.append(out.sum(dim=1).detach().cpu().tolist())\n",
    "newouts = []\n",
    "for batch in outs:\n",
    "    newouts += batch\n",
    "\n",
    "df = pd.DataFrame(ldhdata.eval_data[:len(newouts)], columns=['addcode', 'daycode', 'Condition', 'response', 'observed'])\n",
    "df[['observed']] = newouts\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "for idx, batch in enumerate(tqdm(train_dl)):\n",
    "    if DEV_FLAG and idx >= 2:\n",
    "        break\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    out = model(input_ids, attention_mask)\n",
    "    outs.append(out.sum(dim=1).detach().cpu().tolist())\n",
    "newouts = []\n",
    "for batch in outs:\n",
    "    newouts += batch\n",
    "df = pd.DataFrame(ldhdata.train_data[:len(newouts)], columns=['response', 'score', 'observed'])\n",
    "df[['observed']] = newouts\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
