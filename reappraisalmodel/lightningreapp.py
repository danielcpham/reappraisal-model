# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/LightningReapp.ipynb (unless otherwise specified).

__all__ = ['LightningReapp', 'get_avg_masked_encoding', 'default_model_name']

# Cell
import pickle
import pandas as pd
import pytorch_lightning as lit
import torch
from pytorch_lightning.metrics.functional import r2score, explained_variance
from torch import nn, optim
from torch.nn import functional as F
from transformers import AutoModel

default_model_name = "distilbert-base-uncased-finetuned-sst-2-english"


class LightningReapp(lit.LightningModule):
    def __init__(self, config, pretrained_model_name=default_model_name):
        super().__init__()

        # Set model hyperparams
        self.lr = config.get("lr", 1e-3)
        self.num_embedding_layers = config.get('num_embedding_layers', 1)
        self.save_hyperparameters()

        # Initialize a pretrained model
        self.bert = AutoModel.from_pretrained(
            pretrained_model_name,
            output_hidden_states = True
        )

        self.feature_dim = self.bert.config.dim

        # Turn off autograd for bert encoder
        for param in self.bert.transformer.layer[:-self.num_embedding_layers].parameters():
            param.requires_grad = False

        self.distance_scorer = nn.Sequential(
            nn.Linear(self.feature_dim * self.num_embedding_layers, self.feature_dim),
            nn.Linear(self.feature_dim, 1),
            nn.ReLU(),
        )

        self.pooler = nn.AdaptiveAvgPool1d(1)



        # define metrics
        self.train_loss = lit.metrics.MeanSquaredError()
        self.val_loss = lit.metrics.MeanSquaredError()

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids, attention_mask, output_hidden_states=True)
        last_hidden_states = output.hidden_states[-self.num_embedding_layers:]
        states_combined = torch.cat(last_hidden_states, dim=2)
        avg = self.pooler(states_combined.permute(0,2,1)).squeeze()
        out = self.distance_scorer(avg).squeeze()
        return out

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

    def training_step(self, batch, batch_idx):
        # destructure batch
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        score = batch["score"]
        # Compute the loss
        output = self(input_ids, attention_mask)
        loss = self.train_loss(output, score)
        self.log("train_loss", loss)
        return {"loss": loss}

    def training_epoch_end(self, outputs):
        avg_loss = torch.stack([x["loss"] for x in outputs]).mean()
        self.log("train_loss", avg_loss)

    # VALIDATION LOOP
    def validation_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        expected = batch["score"]
        output = self(input_ids, attention_mask)
        observed = output
        loss = self.val_loss(observed, expected)
        return {
            "val_loss": loss,
            'r2score': r2score(observed, expected),
            'explained_var': explained_variance(observed, expected)
        }

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        r2score = torch.stack([x["r2score"] for x in outputs]).mean()
        explained_var = torch.stack([x["explained_var"] for x in outputs]).mean()

        # calculate spearman's r and pearson's r
        self.log("val_loss", avg_loss)
        self.log('r2score', r2score.item())
        self.log('explained_var', explained_var.item())


    # TESTING LOOP
    def test_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        output = self(input_ids, attention_mask)
        # Eval step
        return {"predict": (batch_idx, output)}

    def test_epoch_end(self, outputs):
        dfs = []
        for output in outputs:
            batch_idx, result = output['predict']
            dfs.append((batch_idx, result.cpu().tolist()))
        with open(f"./output_reapp.pkl", 'wb+') as f:
            pickle.dump(dfs, f)


def get_avg_masked_encoding(state: torch.Tensor, attention_mask: torch.Tensor):
    """[summary]
    For B = batch size, L = encoding length, F = feature vector:
    Args:
        state (torch.Tensor): (B, L, F)
        attention_mask (torch.Tensor): (B, L)
    Returns:
        torch.Tensor: (B, F), where L has been masked by `attention_mask` and then averaged.
            Each vector in the batch represents the average feature vector for the masked_encoding
    """
    encodings = state.unbind(dim=0)  # split the batch up
    new_tensors = []
    # TODO: VECTORIZE
    for i in range(len(encodings)):
        # for each element in the encoding dimension, get the elements that aren't padding using the attention_mask
        encoding_mask = attention_mask[i]
        # Find the indices where attention_mask > 0 (where the actual tokens are without padding)
        indices = encoding_mask.nonzero(as_tuple=True)[0]
        masked_encoding = (
            encodings[i].index_select(0, indices).squeeze()
        )  # torch.Size([len(indices), F])
        avg_feature = torch.mean(masked_encoding, dim=0)
        new_tensors.append(avg_feature)

    return torch.stack(new_tensors)