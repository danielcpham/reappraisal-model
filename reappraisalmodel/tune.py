# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/Tuner.ipynb (unless otherwise specified).

__all__ = ['train_tune', 'run_tune', 'callback_tuner', 'default_tune_config']

# Cell
# export
from functools import partial
import torch
import pytorch_lightning as lit
from ray.tune.integration.pytorch_lightning import TuneReportCallback
from ray import tune

from .lightningreapp import LightningReapp

callback_tuner = TuneReportCallback(
    {
        "loss": "val_loss",
        # "mean_accuracy": "val_accuracy"
    },
    on="validation_end",
)

default_tune_config = {
    "lr": tune.loguniform(1e-4, 1e-1),
    "hidden_layer_size": 50,
}

### TUNING HYPERPARAMETERS
def train_tune(config, ldhdata, epochs=10):
    gpus = 1 if torch.cuda.is_available() else None
    model = LightningReapp(config)
    trainer = lit.Trainer(
        num_folds=3,
        fast_dev_run=1,
        max_epochs=epochs,
        gpus=gpus,
        progress_bar_refresh_rate=30,
        callbacks=[callback_tuner],
    )
    trainer.fit(model, ldhdata)


def run_tune(tune_config=default_tune_config, num_samples=2):
    tune.run(train_tune, config=default_tune_config, num_samples=num_samples)

# https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929
