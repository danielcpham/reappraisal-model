{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('reapp-spacy': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "1893e3c64ced71b15baf27b6686afc2ee07d1dd4176f93ff04c8330708c69b1c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reappraisal Training For Linguistic Distancing and Emotion Regulation\n",
    "\n",
    "## Setup\n",
    "1. Create virtual environment and download required packages (use pipenv).\n",
    "\n",
    "**Notes**\n",
    "- Attention: which words are important for the decoder to focus on at a specific timestep?\n",
    "    - Q = Query\n",
    "    - K = Key\n",
    "    - V = Value\n",
    "- Self-attention: What if Q and K are both the same sentence.\n",
    "- Multi-head Self-Attention: self-attention calculated independently and concurrently (allows transformers to learn representations at different positional encodings)\n",
    "\n",
    "**Sources**\n",
    "-  [Sentiment Analysis Text Classification Tutorial](https://www.youtube.com/watch?v=8N-nM3QW7O0)\n",
    "- [Using Catalyst for Training Organization](https://github.com/catalyst-team/catalyst)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Open in Colab Button\n",
    "# TODO: Write scripts for running as CLI in pipfile\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import runutils\n",
    "import reappDataLoader\n",
    "import SentimentClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and environment setup\n",
    "#TODO: Set up env files for dev and \"prod\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # macOS incompatible with NVIDIA GPUs\n",
    "#Casing can matter for sentiment analysis (\"bad\" vs. \"BAD\")\n",
    "PRETRAINED_MODEL_NAME = 'distilbert-base-cased' \n",
    "RANDOM_SEED = 42\n",
    "MAX_LEN = 160\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load, preprocess, and encode data\n",
    "dataset = load_dataset('imdb')\n",
    "train = dataset['train'].select(range(30))\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "\n",
    "encoded_train_dataset = train.map(lambda batch: tokenizer(batch['text'], add_special_tokens=True, padding=True, truncation=True), batched=True)\n",
    "encoded_train_dataset.set_format(type='torch', output_all_columns=True)\n",
    "encoded_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "from transformers import DistilBertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from reappDataLoader import SentimentClassifier\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#TODO: Change num_training steps to get the length from a dataloader object?\n",
    "model = SentimentClassifier(5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "train_dataloader = DataLoader(encoded_train, batch_size=BATCH_SIZE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(encoded_train) * EPOCHS)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "history = ddict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'epoch {epoch+1} / {EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = run_epoch(\n",
    "        model,\n",
    "        dataloader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train)\n",
    "    )\n",
    "\n",
    "    print(f'Loss: {train_loss}, Accuracy: {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    print(f'Val: Loss: {val_loss}, Accuracy: {val_acc}')\n",
    "    print()\n",
    "\n",
    "history['train_acc'].append(train_acc)\n",
    "history['train_loss'].append(train_loss)\n",
    "history['val_acc'].append(val_acc)\n",
    "history['val_loss'].append(val_loss)\n",
    "if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}